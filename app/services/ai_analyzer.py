"""
AIë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” ëª¨ë“ˆ (Gemini API ì‚¬ìš©)
"""
import os
import logging
import asyncio
from typing import List, Dict, Any, Optional
import google.generativeai as genai
from dotenv import load_dotenv
from datetime import datetime, timedelta
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func
from app.core.database import AsyncSessionLocal
from app.core.models import CollectedItem, AnalysisResult

load_dotenv()
logger = logging.getLogger("hourly_pulse")

# Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
gemini_model = None
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    # gemini-2.0-flash-lite ì‚¬ìš©
    gemini_model = genai.GenerativeModel('gemini-2.0-flash-lite')
    logger.info("âœ… Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (gemini-2.0-flash-lite)")
else:
    logger.warning("âš ï¸ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. AI ë¶„ì„ ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


async def analyze_text_with_ai(text: str, analysis_type: str = "summary") -> Optional[Dict[str, Any]]:
    """
    Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        text: ë¶„ì„í•  í…ìŠ¤íŠ¸
        analysis_type: ë¶„ì„ íƒ€ì… ("summary", "keywords", "sentiment")
    
    Returns:
        ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    if not gemini_model:
        logger.error("âŒ Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    
    if not text or len(text.strip()) < 10:
        logger.warning("âš ï¸ ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì˜ì–´ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© - ì•ˆì „ í•„í„° íšŒí”¼)
        if analysis_type == "summary":
            prompt = f"""You are an expert multi-source trend analyst with deep understanding of current events, market dynamics, social media trends, developer communities, and information patterns across all platforms. Your role is to identify what information is becoming an issue RIGHT NOW and explain WHY it matters at this moment.

## Your Role (Persona):
- Multi-Source Trend Analyst: You analyze trends across news, social media (Reddit, Twitter/X), developer communities (GitHub), video platforms (YouTube), and other sources
- Technology Trend Analyst: You understand tech industry dynamics, emerging patterns, and market shifts
- Social Media Analyst: You recognize viral trends, community discussions, and grassroots movements
- Developer Community Analyst: You understand open-source trends, technical discussions, and developer sentiment
- News Editor: You can identify what's newsworthy and why certain topics gain traction
- Context Interpreter: You connect dots between events, trends, and their significance across different platforms

## Your Reasoning Framework:
When analyzing information, think through these dimensions:

1. **Temporal Significance (Why Now?)**: 
   - What makes this information relevant RIGHT NOW?
   - Is this a sudden development, breaking news, or emerging trend?
   - What changed recently that makes this important?

2. **Context & Background**:
   - What is the broader context behind this issue?
   - What events or trends led to this moment?
   - What background information is needed to understand why this matters?

3. **Impact & Implications**:
   - Who is affected by this issue?
   - What are the potential consequences or implications?
   - How might this develop in the near future?

4. **Pattern Recognition**:
   - Is this part of a larger trend or pattern?
   - How does this relate to other current issues?
   - What makes this stand out from similar past events?

## Analysis Task:
Analyze the following information from various sources (news, social media, GitHub, YouTube, etc.). Identify the main issues that are becoming important RIGHT NOW, not just frequently mentioned keywords. Consider all source types equally - each provides valuable insights.

{text[:32000]}  # 6000 -> 32000ìœ¼ë¡œ ì¦ê°€ (4M TPM í™œìš©)

For each issue you identify, provide:
1. **Issue Title**: A descriptive, meaningful title (not just a single word)
2. **What It Is**: Brief description of what the issue is about
3. **Why Now**: Explain WHY this is becoming an issue RIGHT NOW - what makes it timely and relevant at this moment
4. **Context**: Provide background context that helps understand why this matters

Format your response as:
Issues:
1. [Descriptive Issue Title]
   What: [Brief description of what this issue is about]
   Why Now: [Explain why this is becoming an issue RIGHT NOW - what changed, what makes it timely]
   Context: [Background context that explains the significance]

2. [Descriptive Issue Title]
   What: [Brief description]
   Why Now: [Why this matters right now]
   Context: [Background context]

3. [Descriptive Issue Title]
   What: [Brief description]
   Why Now: [Why this matters right now]
   Context: [Background context]

Summary: [Overall summary of the main trends and why they matter now, one sentence under 200 characters]
Keywords: [5-10 relevant keywords, comma-separated]

Example:
Issues:
1. AI Safety Regulation Push
   What: Major tech companies and governments are pushing for AI safety regulations as AI capabilities rapidly advance
   Why Now: Recent high-profile AI incidents and rapid deployment of powerful AI models have created urgency for regulatory frameworks before potential risks materialize
   Context: This follows months of AI breakthroughs and growing public concern about AI's societal impact, making it a critical policy moment

2. Climate Tech Investment Surge
   What: Significant increase in climate technology investments and carbon reduction commitments
   Why Now: Recent climate events and policy changes have created a window of opportunity for climate tech, with investors seeing both urgency and potential returns
   Context: This aligns with upcoming climate summits and new government incentives, creating a convergence of factors that make climate tech attractive now

Summary: Current focus is on AI regulation urgency and climate tech investment surge, both driven by recent developments creating critical decision points.
Keywords: AI, regulation, safety, climate, tech, investment, policy, urgency
"""
        elif analysis_type == "keywords":
            prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œì™€ ì£¼ì œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text[:16000]}  # 4000 -> 16000ìœ¼ë¡œ ì¦ê°€ (4M TPM í™œìš©)

ì‘ë‹µ í˜•ì‹:
- í‚¤ì›Œë“œ: (ì¤‘ìš”í•œ í‚¤ì›Œë“œ 10ê°œ, ì‰¼í‘œë¡œ êµ¬ë¶„)
- ì£¼ìš” ì£¼ì œ: (3-5ê°œì˜ ì£¼ìš” ì£¼ì œ)
"""
        else:  # sentiment
            prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text[:16000]}  # 4000 -> 16000ìœ¼ë¡œ ì¦ê°€ (4M TPM í™œìš©)

ì‘ë‹µ í˜•ì‹:
- ê°ì •: (positive, negative, neutral ì¤‘ í•˜ë‚˜)
- ì´ìœ : (ê°„ë‹¨í•œ ì„¤ëª…)
"""
        
        # Gemini API í˜¸ì¶œ (ë¹„ë™ê¸° ì‹¤í–‰ì„ ìœ„í•´ run_in_executor ì‚¬ìš©)
        # ì•ˆì „ ì„¤ì •ì„ ìµœëŒ€í•œ ì™„í™”
        safety_settings = [
            {
                "category": genai.types.HarmCategory.HARM_CATEGORY_HARASSMENT,
                "threshold": genai.types.HarmBlockThreshold.BLOCK_NONE
            },
            {
                "category": genai.types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                "threshold": genai.types.HarmBlockThreshold.BLOCK_NONE
            },
            {
                "category": genai.types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                "threshold": genai.types.HarmBlockThreshold.BLOCK_NONE
            },
            {
                "category": genai.types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                "threshold": genai.types.HarmBlockThreshold.BLOCK_NONE
            },
        ]
        
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            lambda: gemini_model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.7,  # ì°½ì˜ì„± ì¦ê°€ (ë§¥ë½ê³¼ ì„¤ëª…ì„ ìœ„í•´)
                    max_output_tokens=8000,  # 4M TPM í™œìš©í•˜ì—¬ ë” ê¸´ ì‘ë‹µ (2000 -> 8000ìœ¼ë¡œ ì¦ê°€)
                ),
                safety_settings=safety_settings
            )
        )
        
        # Gemini ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        try:
            # finish_reason í™•ì¸
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                finish_reason = candidate.finish_reason
                
                # finish_reasonì´ SAFETY(2) ë˜ëŠ” ë‹¤ë¥¸ ì´ìœ ë¡œ ì°¨ë‹¨ëœ ê²½ìš°
                if finish_reason != 1:  # 1 = STOP (ì •ìƒ ì™„ë£Œ)
                    logger.warning(f"âš ï¸ Gemini ì‘ë‹µì´ ì°¨ë‹¨ë¨ (finish_reason: {finish_reason})")
                    if finish_reason == 2:  # SAFETY
                        logger.warning("  ì•ˆì „ í•„í„°ì— ì˜í•´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ì •í•´ì£¼ì„¸ìš”.")
                    return None
                
                # ì •ìƒ ì‘ë‹µì¸ ê²½ìš° í…ìŠ¤íŠ¸ ì¶”ì¶œ
                if hasattr(candidate, 'content') and candidate.content.parts:
                    content = candidate.content.parts[0].text
                else:
                    logger.error("âŒ Gemini ì‘ë‹µì— í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return None
            elif hasattr(response, 'text'):
                content = response.text
            else:
                logger.error("âŒ Gemini ì‘ë‹µ í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤.")
                return None
        except Exception as e:
            logger.error(f"âŒ Gemini ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
        
        logger.info(f"âœ… AI ë¶„ì„ ì™„ë£Œ ({analysis_type})")
        
        # ì‘ë‹µ íŒŒì‹±
        return parse_ai_response(content, analysis_type)
        
    except Exception as e:
        logger.error(f"âŒ AI ë¶„ì„ ì‹¤íŒ¨: {type(e).__name__} - {e}")
        import traceback
        traceback.print_exc()
        return None


def parse_ai_response(content: str, analysis_type: str) -> Dict[str, Any]:
    """
    AI ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    result = {}
    
    try:
        # ì˜ì–´ ì‘ë‹µ íŒŒì‹± (Issues:, Summary:, Keywords: í˜•ì‹)
        if 'Issues:' in content or 'Summary:' in content or 'Keywords:' in content:
            lines = content.split('\n')
            issues = []
            current_issue = None
            current_what = None
            current_why_now = None
            current_context = None
            
            for i, line in enumerate(lines):
                line = line.strip()
                if not line:
                    continue
                
                # Issues: ì„¹ì…˜ ì‹œì‘
                if line.startswith('Issues:'):
                    continue
                
                # ì´ìŠˆ í•­ëª© íŒŒì‹± (ìƒˆ í˜•ì‹: 1. [Title] ë˜ëŠ” ê¸°ì¡´ í˜•ì‹: 1. [Title]: [Description])
                if line and line[0].isdigit() and ('.' in line or ':' in line):
                    # ì´ì „ ì´ìŠˆ ì €ì¥
                    if current_issue:
                        issues.append({
                            'title': current_issue,
                            'description': current_what or current_context or '',
                            'what': current_what or '',
                            'why_now': current_why_now or '',
                            'context': current_context or ''
                        })
                    
                    # ìƒˆ ì´ìŠˆ ì‹œì‘
                    issue_text = line.split('.', 1)[1].strip() if '.' in line else line
                    if ':' in issue_text:
                        # ê¸°ì¡´ í˜•ì‹: 1. [Title]: [Description]
                        parts = issue_text.split(':', 1)
                        current_issue = parts[0].strip()
                        current_what = parts[1].strip()
                        current_why_now = None
                        current_context = None
                    else:
                        # ìƒˆ í˜•ì‹: 1. [Title] (ë‹¤ìŒ ì¤„ì— What:, Why Now:, Context:)
                        current_issue = issue_text.strip()
                        current_what = None
                        current_why_now = None
                        current_context = None
                
                # What: ì„¹ì…˜
                elif line.startswith('What:') and current_issue:
                    current_what = line.replace('What:', '').strip()
                
                # Why Now: ì„¹ì…˜
                elif line.startswith('Why Now:') and current_issue:
                    current_why_now = line.replace('Why Now:', '').strip()
                
                # Context: ì„¹ì…˜
                elif line.startswith('Context:') and current_issue:
                    current_context = line.replace('Context:', '').strip()
                
                # Topics: í˜•ì‹ (í•˜ìœ„ í˜¸í™˜ì„±)
                elif line.startswith('Topics:'):
                    topics_str = line.replace('Topics:', '').strip()
                    result['topics'] = [t.strip() for t in topics_str.split(',') if t.strip()]
                
                # Summary: í˜•ì‹
                elif line.startswith('Summary:'):
                    summary = line.replace('Summary:', '').strip()
                    result['summary'] = summary
                
                # Keywords: í˜•ì‹
                elif line.startswith('Keywords:'):
                    keywords_str = line.replace('Keywords:', '').strip()
                    result['keywords'] = [k.strip() for k in keywords_str.split(',') if k.strip()]
            
            # ë§ˆì§€ë§‰ ì´ìŠˆ ì €ì¥
            if current_issue:
                issues.append({
                    'title': current_issue,
                    'description': current_what or current_context or '',
                    'what': current_what or '',
                    'why_now': current_why_now or '',
                    'context': current_context or ''
                })
            
            # ì´ìŠˆë¥¼ topicsë¡œ ë³€í™˜ (ì´ìŠˆ ì œëª©ë§Œ)
            if issues:
                result['issues'] = issues
                result['topics'] = [issue['title'] for issue in issues]
                # ì²« ë²ˆì§¸ ì´ìŠˆì˜ ì„¤ëª…ì„ ì „ì²´ summaryë¡œ ì‚¬ìš© (ì—†ëŠ” ê²½ìš°)
                if 'summary' not in result or not result.get('summary'):
                    # why_nowê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ summaryë¡œ ì‚¬ìš©
                    if issues[0].get('why_now'):
                        result['summary'] = issues[0]['why_now']
                    elif issues[0].get('what'):
                        result['summary'] = issues[0]['what']
                    else:
                        result['summary'] = issues[0].get('description', '')
        
        # í•œêµ­ì–´ ì‘ë‹µ íŒŒì‹±
        else:
            lines = content.split('\n')
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # ì„¹ì…˜ í—¤ë” ê°ì§€
                if ':' in line:
                    parts = line.split(':', 1)
                    if len(parts) == 2:
                        key = parts[0].strip().lower()
                        value = parts[1].strip()
                        
                        if 'ì£¼ìš” ì´ìŠˆ' in key or 'ì£¼ìš” ì£¼ì œ' in key or 'topics' in key:
                            result['topics'] = [t.strip() for t in value.split(',') if t.strip()]
                        elif 'ìš”ì•½' in key or 'summary' in key:
                            result['summary'] = value
                        elif 'í‚¤ì›Œë“œ' in key or 'keywords' in key:
                            keywords = [k.strip() for k in value.split(',') if k.strip()]
                            result['keywords'] = keywords
                        elif 'ê°ì •' in key or 'sentiment' in key:
                            sentiment = value.lower()
                            if 'positive' in sentiment or 'ê¸ì •' in sentiment:
                                result['sentiment'] = 'positive'
                            elif 'negative' in sentiment or 'ë¶€ì •' in sentiment:
                                result['sentiment'] = 'negative'
                            else:
                                result['sentiment'] = 'neutral'
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        if 'keywords' not in result:
            # Keywordsê°€ ì—†ìœ¼ë©´ Summaryì—ì„œ ì¶”ì¶œ ì‹œë„
            if 'summary' in result and result['summary']:
                # Summaryì—ì„œ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œ ì¶”ì¶œ
                summary = result['summary']
                if ',' in summary:
                    result['keywords'] = [k.strip() for k in summary.split(',')[:10] if k.strip()]
                else:
                    result['keywords'] = []
            else:
                result['keywords'] = []
        
        if 'summary' not in result:
            result['summary'] = content[:200] if content else "ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        if 'topics' not in result:
            # Topicsê°€ ì—†ìœ¼ë©´ Issuesì—ì„œ ì¶”ì¶œ, ì—†ìœ¼ë©´ Keywordsì—ì„œ ì¶”ì¶œ
            if result.get('issues'):
                result['topics'] = [issue['title'] for issue in result['issues']]
            elif result.get('keywords'):
                result['topics'] = result['keywords'][:5]
            else:
                result['topics'] = []
        
        if 'sentiment' not in result:
            result['sentiment'] = 'neutral'
            
    except Exception as e:
        logger.error(f"âŒ AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        result = {
            'summary': content[:200] if content else "ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            'keywords': [],
            'topics': [],
            'sentiment': 'neutral'
        }
    
    return result


async def estimate_news_interest_score(title: str, description: str = "") -> Optional[int]:
    """
    AIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ê´€ì‹¬ë„ ì ìˆ˜ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.
    
    Args:
        title: ë‰´ìŠ¤ ì œëª©
        description: ë‰´ìŠ¤ ì„¤ëª… (ì„ íƒì )
    
    Returns:
        ì¶”ì • ì¡°íšŒìˆ˜ (0-100000 ë²”ìœ„ì˜ ì ìˆ˜ë¥¼ ì¡°íšŒìˆ˜ë¡œ ë³€í™˜)
    """
    if not gemini_model:
        logger.warning("âš ï¸ Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‰´ìŠ¤ ê´€ì‹¬ë„ ì¶”ì •ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None
    
    if not title or len(title.strip()) < 5:
        return None
    
    try:
        # ë‰´ìŠ¤ ê´€ì‹¬ë„ í‰ê°€ í”„ë¡¬í”„íŠ¸
        text_content = f"Title: {title}"
        if description and len(description.strip()) > 10:
            text_content += f"\nDescription: {description[:500]}"
        
        prompt = f"""You are an expert news analyst. Evaluate the potential public interest and viewership for this news article based on its title and description.

Consider these factors:
1. **Newsworthiness**: How important or significant is this news?
2. **Timeliness**: Is this breaking news or a current hot topic?
3. **Relevance**: How relevant is this to a broad audience?
4. **Impact**: How many people would be affected or interested?
5. **Viral Potential**: How likely is this to be shared or discussed?

News Article:
{text_content}

Provide your assessment as a single number from 0 to 100, where:
- 0-20: Low interest (niche topic, limited relevance)
- 21-40: Moderate interest (somewhat relevant)
- 41-60: Good interest (relevant to many people)
- 61-80: High interest (important news, breaking story)
- 81-100: Very high interest (major breaking news, viral potential, widespread impact)

Respond with ONLY a number between 0 and 100, nothing else."""
        
        # Gemini API í˜¸ì¶œ
        safety_settings = [
            {
                "category": genai.types.HarmCategory.HARM_CATEGORY_HARASSMENT,
                "threshold": genai.types.HarmBlockThreshold.BLOCK_NONE
            },
            {
                "category": genai.types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                "threshold": genai.types.HarmBlockThreshold.BLOCK_NONE
            },
            {
                "category": genai.types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                "threshold": genai.types.HarmBlockThreshold.BLOCK_NONE
            },
            {
                "category": genai.types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                "threshold": genai.types.HarmBlockThreshold.BLOCK_NONE
            },
        ]
        
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            lambda: gemini_model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.3,  # ë‚®ì€ ì˜¨ë„ë¡œ ì¼ê´€ì„± ìˆëŠ” ì ìˆ˜
                    max_output_tokens=10,  # ìˆ«ìë§Œ í•„ìš”í•˜ë¯€ë¡œ ì§§ê²Œ
                ),
                safety_settings=safety_settings
            )
        )
        
        # ì‘ë‹µ íŒŒì‹±
        try:
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                if candidate.finish_reason == 1:  # STOP (ì •ìƒ ì™„ë£Œ)
                    if hasattr(candidate, 'content') and candidate.content.parts:
                        content = candidate.content.parts[0].text.strip()
                    elif hasattr(response, 'text'):
                        content = response.text.strip()
                    else:
                        return None
                    
                    # ìˆ«ìë§Œ ì¶”ì¶œ
                    import re
                    numbers = re.findall(r'\d+', content)
                    if numbers:
                        score = int(numbers[0])
                        # 0-100 ë²”ìœ„ë¡œ ì œí•œ
                        score = max(0, min(100, score))
                        # ì ìˆ˜ë¥¼ ì¡°íšŒìˆ˜ë¡œ ë³€í™˜ (0-100 ì ìˆ˜ë¥¼ 100-10000 ì¡°íšŒìˆ˜ë¡œ ë³€í™˜)
                        estimated_views = 100 + (score * 99)  # 100 ~ 10000 ë²”ìœ„
                        return estimated_views
        except Exception as e:
            logger.warning(f"âš ï¸ ë‰´ìŠ¤ ê´€ì‹¬ë„ ì ìˆ˜ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
        
        return None
        
    except Exception as e:
        logger.warning(f"âš ï¸ ë‰´ìŠ¤ ê´€ì‹¬ë„ ì¶”ì • ì‹¤íŒ¨: {type(e).__name__} - {e}")
        return None


async def get_recent_items_for_analysis(hours: int = 1, limit: int = 1000) -> List[CollectedItem]:  # 100 -> 1000ìœ¼ë¡œ ì¦ê°€ (4M TPM í™œìš©)
    """
    ë¶„ì„í•  ìµœê·¼ ìˆ˜ì§‘ ì•„ì´í…œì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    ëª¨ë“  ì†ŒìŠ¤ íƒ€ì…ì„ ê· ë“±í•˜ê²Œ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘ì„± í™•ë³´
    
    Args:
        hours: ìµœê·¼ ëª‡ ì‹œê°„ ë‚´ ë°ì´í„°
        limit: ìµœëŒ€ ê°œìˆ˜
    
    Returns:
        CollectedItem ë¦¬ìŠ¤íŠ¸ (ì†ŒìŠ¤ ë‹¤ì–‘ì„±ì„ ê³ ë ¤í•œ ìƒ˜í”Œë§)
    """
    async with AsyncSessionLocal() as session:
        try:
            from datetime import timezone
            import random
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours)
            
            # 1. ëª¨ë“  ì†ŒìŠ¤ íƒ€ì…ë³„ë¡œ ë°ì´í„° ê°œìˆ˜ í™•ì¸
            source_type_query = select(
                CollectedItem.source_type,
                func.count(CollectedItem.id)
            ).where(
                CollectedItem.collected_at >= cutoff_time
            ).group_by(CollectedItem.source_type)
            
            source_type_result = await session.execute(source_type_query)
            source_type_counts = dict(source_type_result.all())
            
            if not source_type_counts:
                logger.warning("âš ï¸ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # 2. ì†ŒìŠ¤ íƒ€ì…ë³„ë¡œ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§ (ìµœì†Œ 10ê°œì”©, ìµœëŒ€ limit/ì†ŒìŠ¤íƒ€ì…ìˆ˜)
            items = []
            source_types = list(source_type_counts.keys())
            items_per_source = max(10, limit // len(source_types))
            
            logger.info(f"ğŸ“Š ì†ŒìŠ¤ íƒ€ì…ë³„ ë°ì´í„° ë¶„í¬: {source_type_counts}")
            
            for source_type in source_types:
                try:
                    # ê° ì†ŒìŠ¤ íƒ€ì…ì—ì„œ ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                    query = select(CollectedItem).where(
                        CollectedItem.collected_at >= cutoff_time,
                        CollectedItem.source_type == source_type
                    ).order_by(CollectedItem.collected_at.desc()).limit(items_per_source * 2)  # 2ë°° ê°€ì ¸ì™€ì„œ ë‹¤ì–‘ì„± í™•ë³´
                    
                    result = await session.execute(query)
                    source_items = list(result.scalars().all())
                    
                    # ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ë‹¤ì–‘ì„± í™•ë³´ (ìµœì‹ ì„±ê³¼ ë‹¤ì–‘ì„± ê· í˜•)
                    if len(source_items) > items_per_source:
                        # ìµœì‹  50%ëŠ” í™•ì‹¤íˆ í¬í•¨, ë‚˜ë¨¸ì§€ 50%ëŠ” ëœë¤ ìƒ˜í”Œë§
                        recent_count = items_per_source // 2
                        recent_items = source_items[:recent_count]
                        random_items = random.sample(source_items[recent_count:], min(items_per_source - recent_count, len(source_items) - recent_count))
                        source_items = recent_items + random_items
                    else:
                        source_items = source_items[:items_per_source]
                    
                    items.extend(source_items)
                    
                    if len(items) >= limit:
                        break
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ {source_type} ì†ŒìŠ¤ íƒ€ì… ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
                    continue
            
            # 3. ìµœì¢…ì ìœ¼ë¡œ limit ê°œìˆ˜ë§Œí¼ë§Œ ë°˜í™˜ (ìµœì‹ ì„± ìš°ì„ )
            items = sorted(items, key=lambda x: x.collected_at if x.collected_at else datetime.min, reverse=True)[:limit]
            
            # ì†ŒìŠ¤ íƒ€ì…ë³„ ìµœì¢… ë¶„í¬ ë¡œê¹…
            final_source_dist = {}
            for item in items:
                final_source_dist[item.source_type] = final_source_dist.get(item.source_type, 0) + 1
            
            logger.info(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ: ìµœê·¼ {hours}ì‹œê°„ ë‚´ {len(items)}ê°œ ì•„ì´í…œ (ì†ŒìŠ¤ ë¶„í¬: {final_source_dist})")
            return items
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {type(e).__name__} - {e}")
            return []


async def prepare_text_for_analysis(items: List[CollectedItem]) -> str:
    """
    ìˆ˜ì§‘ëœ ì•„ì´í…œë“¤ì„ ë¶„ì„ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ëª¨ë“  ì†ŒìŠ¤ íƒ€ì…ì„ í¬í•¨í•˜ì—¬ ë” ë‹¤ì–‘í•œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    texts = []
    
    # ìµœëŒ€ 1000ê°œê¹Œì§€ ì‚¬ìš© (4M TPM í™œìš©í•˜ì—¬ ë” ë§ì€ ë°ì´í„°ë¡œ ë¶„ì„ ì •í™•ë„ í–¥ìƒ)
    # ì†ŒìŠ¤ íƒ€ì…ë³„ë¡œ ê· ë“±í•˜ê²Œ í¬í•¨
    max_items = min(1000, len(items))  # 100 -> 1000ìœ¼ë¡œ ì¦ê°€
    selected_items = items[:max_items]
    
    # ì†ŒìŠ¤ íƒ€ì…ë³„ í†µê³„
    source_stats = {}
    for item in selected_items:
        source_stats[item.source_type] = source_stats.get(item.source_type, 0) + 1
    
    logger.info(f"ğŸ“ ë¶„ì„ í…ìŠ¤íŠ¸ ì¤€ë¹„: {len(selected_items)}ê°œ ì•„ì´í…œ (ì†ŒìŠ¤ ë¶„í¬: {source_stats})")
    
    for item in selected_items:
        # ì œëª©ê³¼ ë‚´ìš© ëª¨ë‘ ì‚¬ìš© (ë‚´ìš© ê¸°ë°˜ ë¶„ì„)
        title = item.title or ""
        content = item.content or ""
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        import html
        title = html.unescape(title)
        content = html.unescape(content)
        
        # íŠ¹ìˆ˜ ë¬¸ì ì œê±° ë° ì •ë¦¬
        title = title.replace('\n', ' ').replace('\r', ' ').strip()
        content = content.replace('\n', ' ').replace('\r', ' ').strip()
        
        # ì†ŒìŠ¤ íƒ€ì… ì •ë³´ í¬í•¨ (ë‹¤ì–‘ì„± ê°•ì¡°)
        source_type_label = item.source_type.upper()
        
        # ì œëª©ê³¼ ë‚´ìš© ì¡°í•© (ë‚´ìš©ì´ ìˆìœ¼ë©´ í¬í•¨, 4M TPM í™œìš©í•˜ì—¬ ë” ê¸´ í…ìŠ¤íŠ¸)
        if content and len(content) > 20:
            # YouTubeëŠ” ì„¤ëª…ì´ ë” ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë” ë§ì€ í…ìŠ¤íŠ¸ ì‚¬ìš©
            if source_type_label == "YOUTUBE":
                # ì œëª© + ë‚´ìš© (ìµœëŒ€ 1000ìë¡œ ì¦ê°€)
                text = f"[{source_type_label}] {title[:150]} | {content[:1000]}"
            else:
                # ì œëª© + ë‚´ìš© ìš”ì•½ (ìµœëŒ€ 500ì)
                text = f"[{source_type_label}] {title[:150]} | {content[:500]}"
        else:
            # ì œëª©ë§Œ ì‚¬ìš©
            text = f"[{source_type_label}] {title[:200]}"
        
        if text and len(text.strip()) > 10:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
            texts.append(text)
    
    final_text = "\n".join(texts)
    logger.info(f"ğŸ“ ë¶„ì„ í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ: {len(texts)}ê°œ í•­ëª©, ì´ {len(final_text)}ì")
    return final_text


async def calculate_importance_score(topic: str, items: List[CollectedItem]) -> float:
    """
    í† í”½ì˜ ì¤‘ìš”ë„ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        topic: í† í”½/í‚¤ì›Œë“œ
        items: ê´€ë ¨ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ì¤‘ìš”ë„ ì ìˆ˜ (0.0 ~ 1.0)
    """
    if not items:
        return 0.0
    
    # 1. ì–¸ê¸‰ íšŸìˆ˜ (ë¹ˆë„)
    mention_count = sum(1 for item in items if topic.lower() in item.title.lower() or 
                       (item.content and topic.lower() in item.content.lower()))
    
    # 2. ì†ŒìŠ¤ ë‹¤ì–‘ì„± (ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ì–¸ê¸‰ë˜ì—ˆëŠ”ì§€)
    unique_sources = len(set(item.source_type for item in items))
    
    # 3. ìµœê·¼ì„± (ìµœê·¼ ë°ì´í„°ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
    from datetime import timezone
    now = datetime.now(timezone.utc)
    recency_score = sum(
        1.0 / (1 + (now - (item.collected_at if item.collected_at.tzinfo else item.collected_at.replace(tzinfo=timezone.utc))).total_seconds() / 3600)
        for item in items
    ) / len(items) if items else 0
    
    # 4. ì†Œì…œ ë¯¸ë””ì–´ ì°¸ì—¬ë„ (upvotes, likes ë“±)
    engagement_score = 0.0
    for item in items:
        if item.extra_data:
            upvotes = item.extra_data.get('upvotes', 0) or 0
            likes = item.extra_data.get('likes', 0) or 0
            views = item.extra_data.get('views', 0) or 0
            engagement_score += (upvotes + likes * 0.5 + views * 0.1) / 1000.0
    
    # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ì •ê·œí™”)
    mention_score = min(mention_count / 10.0, 1.0)  # ìµœëŒ€ 10íšŒ ì–¸ê¸‰ = 1.0
    diversity_score = min(unique_sources / 5.0, 1.0)  # ìµœëŒ€ 5ê°œ ì†ŒìŠ¤ = 1.0
    recency_normalized = min(recency_score, 1.0)
    engagement_normalized = min(engagement_score / len(items) if items else 0, 1.0)
    
    # ê°€ì¤‘ í‰ê· 
    importance = (
        mention_score * 0.3 +
        diversity_score * 0.3 +
        recency_normalized * 0.2 +
        engagement_normalized * 0.2
    )
    
    return min(importance, 1.0)


async def analyze_collected_data(hours: int = 1) -> List[Dict[str, Any]]:
    """
    ìµœê·¼ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        hours: ë¶„ì„í•  ìµœê·¼ ì‹œê°„ ë²”ìœ„
    
    Returns:
        ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    if not gemini_model:
        logger.warning("âš ï¸ Gemini API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ AI ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []
    
    logger.info("ğŸ¤– AI ë¶„ì„ ì‹œì‘...")
    
    # 1. ìµœê·¼ ìˆ˜ì§‘ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    items = await get_recent_items_for_analysis(hours=hours, limit=100)
    
    if not items:
        logger.warning("âš ï¸ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    # 2. í…ìŠ¤íŠ¸ ì¤€ë¹„ (ë‚´ìš© í¬í•¨)
    analysis_text = await prepare_text_for_analysis(items)
    
    if len(analysis_text) < 50:
        logger.warning("âš ï¸ ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
        return []
    
    # 3. AI ë¶„ì„ ìˆ˜í–‰ (ë‚´ìš© ê¸°ë°˜ ì´ìŠˆ ì¶”ì¶œ)
    ai_result = await analyze_text_with_ai(analysis_text, analysis_type="summary")
    
    if not ai_result:
        logger.error("âŒ AI ë¶„ì„ ì‹¤íŒ¨")
        return []
    
    # 4. ê° í† í”½ì— ëŒ€í•´ ìƒì„¸ ë¶„ì„
    analysis_results = []
    
    # ì´ìŠˆ ê¸°ë°˜ ë¶„ì„ (ë‚´ìš© ì¤‘ì‹¬)
    issues = ai_result.get('issues', [])
    topics = ai_result.get('topics', [])
    
    # Issuesê°€ ì—†ìœ¼ë©´ topicsë¥¼ ì´ìŠˆë¡œ ë³€í™˜
    if not issues and topics:
        issues = [{'title': topic, 'description': ''} for topic in topics[:5]]
    
    # í‚¤ì›Œë“œë§Œ ìˆëŠ” ê²½ìš° (í•˜ìœ„ í˜¸í™˜ì„±)
    if not issues and not topics:
        keywords = ai_result.get('keywords', [])[:5]
        issues = [{'title': kw, 'description': ''} for kw in keywords]
    
    for issue in issues[:10]:  # ìµœëŒ€ 10ê°œ ì´ìŠˆë§Œ ë¶„ì„
        issue_title = issue.get('title', '') if isinstance(issue, dict) else str(issue)
        issue_desc = issue.get('description', '') if isinstance(issue, dict) else ''
        
        if not issue_title:
            continue
        
        # í•´ë‹¹ ì´ìŠˆì™€ ê´€ë ¨ëœ ì•„ì´í…œ í•„í„°ë§ (ë‚´ìš© ê¸°ë°˜ ë§¤ì¹­)
        # ì†ŒìŠ¤ íƒ€ì…ë³„ë¡œ ê· ë“±í•˜ê²Œ í¬í•¨í•˜ì—¬ ì¶œì²˜ ë‹¤ì–‘ì„± í™•ë³´
        related_items = []
        issue_keywords = issue_title.lower().split()
        
        # ì†ŒìŠ¤ íƒ€ì…ë³„ë¡œ ì•„ì´í…œ ë¶„ë¥˜
        items_by_source = {}
        for item in items:
            source_type = item.source_type or 'unknown'
            if source_type not in items_by_source:
                items_by_source[source_type] = []
            items_by_source[source_type].append(item)
        
        # ê° ì†ŒìŠ¤ íƒ€ì…ë³„ë¡œ ìµœì†Œ 1ê°œ ì´ìƒ í¬í•¨í•˜ë„ë¡ ë§¤ì¹­
        for source_type, source_items in items_by_source.items():
            source_matched = []
            
            for item in source_items:
                title_lower = (item.title or "").lower()
                content_lower = (item.content or "").lower()
                
                # ì´ìŠˆ ì œëª©ì˜ ì£¼ìš” ë‹¨ì–´ë“¤ì´ ì œëª©ì´ë‚˜ ë‚´ìš©ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
                match_score = 0
                for keyword in issue_keywords:
                    if len(keyword) > 2:  # 2ê¸€ì ì´ìƒì¸ í‚¤ì›Œë“œë„ í¬í•¨ (ê¸°ì¡´ 3ê¸€ì â†’ 2ê¸€ì)
                        if keyword in title_lower:
                            match_score += 2
                        elif keyword in content_lower:
                            match_score += 1
                
                # ìµœì†Œ 1ì  ì´ìƒì´ë©´ ê´€ë ¨ ì•„ì´í…œìœ¼ë¡œ ê°„ì£¼ (ê¸°ì¡´ 2ì  â†’ 1ì ìœ¼ë¡œ ì™„í™”)
                if match_score >= 1:
                    source_matched.append((item, match_score))
            
            # ë§¤ì¹­ì´ ì•ˆ ë˜ë©´ ì´ìŠˆ ì œëª© ìì²´ê°€ í¬í•¨ëœ ê²½ìš°ë„ í¬í•¨
            if not source_matched:
                for item in source_items:
                    title_lower = (item.title or "").lower()
                    content_lower = (item.content or "").lower()
                    if issue_title.lower() in title_lower or issue_title.lower() in content_lower:
                        source_matched.append((item, 3))  # ë†’ì€ ì ìˆ˜ ë¶€ì—¬
            
            # ê° ì†ŒìŠ¤ íƒ€ì…ë³„ë¡œ ìµœëŒ€ 10ê°œê¹Œì§€ ì ìˆ˜ ìˆœìœ¼ë¡œ ì„ íƒ
            source_matched.sort(key=lambda x: x[1], reverse=True)
            related_items.extend([item for item, score in source_matched[:10]])
        
        # ì „ì²´ ì•„ì´í…œì—ì„œë„ ì¶”ê°€ ë§¤ì¹­ ì‹œë„ (ì†ŒìŠ¤ íƒ€ì… ë¬´ê´€)
        if len(related_items) < 5:
            for item in items:
                if item in related_items:
                    continue
                title_lower = (item.title or "").lower()
                content_lower = (item.content or "").lower()
                if issue_title.lower() in title_lower or issue_title.lower() in content_lower:
                    related_items.append(item)
        
        if not related_items:
            continue
        
        # ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
        importance_score = await calculate_importance_score(issue_title, related_items)
        
        # ê´€ë ¨ ì•„ì´í…œ ID ìˆ˜ì§‘
        related_ids = [item.id for item in related_items]
        
        # ë¶„ì„ ê²°ê³¼ ìƒì„± (ì´ìŠˆ ì„¤ëª… í¬í•¨)
        issue_what = issue.get('what', '') if isinstance(issue, dict) else ''
        issue_why_now = issue.get('why_now', '') if isinstance(issue, dict) else ''
        issue_context = issue.get('context', '') if isinstance(issue, dict) else ''
        
        # summaryëŠ” why_nowê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ì¡´ ë¡œì§
        summary_text = issue_why_now if issue_why_now else (issue_desc if issue_desc else ai_result.get('summary', ''))
        
        analysis_result = {
            'analysis_type': 'comprehensive',
            'topic': issue_title,  # ì´ìŠˆ ì œëª©ì„ í† í”½ìœ¼ë¡œ ì‚¬ìš©
            'summary': summary_text,
            'keywords': ai_result.get('keywords', []),
            'sentiment': ai_result.get('sentiment', 'neutral'),
            'importance_score': importance_score,
            'source_count': len(set(item.source_type for item in related_items)),
            'collected_item_ids': related_ids,
            'what': issue_what,
            'why_now': issue_why_now,
            'context': issue_context
        }
        
        analysis_results.append(analysis_result)
    
    logger.info(f"âœ… AI ë¶„ì„ ì™„ë£Œ: {len(analysis_results)}ê°œ í† í”½ ë¶„ì„ë¨")
    
    return analysis_results


async def save_analysis_results(analysis_results: List[Dict[str, Any]]) -> int:
    """
    ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        analysis_results: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ì €ì¥ëœ ê²°ê³¼ ìˆ˜
    """
    if not analysis_results:
        return 0
    
    saved_count = 0
    async with AsyncSessionLocal() as session:
        try:
            for result in analysis_results:
                # ì¤‘ë³µ ì²´í¬ (ê°™ì€ í† í”½ì´ ìµœê·¼ 1ì‹œê°„ ë‚´ì— ë¶„ì„ë˜ì—ˆëŠ”ì§€)
                one_hour_ago = datetime.now() - timedelta(hours=1)
                existing = await session.execute(
                    select(AnalysisResult).where(
                        AnalysisResult.topic == result['topic'],
                        AnalysisResult.analyzed_at >= one_hour_ago
                    )
                )
                if existing.scalar_one_or_none():
                    continue  # ì´ë¯¸ ìµœê·¼ì— ë¶„ì„ë¨
                
                # AnalysisResult ìƒì„±
                analysis_result = AnalysisResult(
                    analysis_type=result.get('analysis_type', 'comprehensive'),
                    topic=result['topic'],
                    summary=result.get('summary', ''),
                    keywords=result.get('keywords', []),
                    sentiment=result.get('sentiment', 'neutral'),
                    importance_score=result.get('importance_score', 0.0),
                    source_count=result.get('source_count', 0),
                    collected_item_ids=result.get('collected_item_ids', []),
                    what=result.get('what', ''),
                    why_now=result.get('why_now', ''),
                    context=result.get('context', '')
                )
                
                session.add(analysis_result)
                saved_count += 1
            
            await session.commit()
            logger.info(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ")
            
        except Exception as e:
            await session.rollback()
            logger.error(f"âŒ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {type(e).__name__} - {e}")
            raise
    
    return saved_count

