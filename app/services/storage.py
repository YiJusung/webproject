"""
ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” ëª¨ë“ˆ
"""
import logging
import sys
import asyncio
import os
import psutil
from datetime import datetime
from typing import List, Dict, Any, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, delete, text
from app.core.database import AsyncSessionLocal
from app.core.models import CollectedItem

logger = logging.getLogger("hourly_pulse")

# ìµœëŒ€ ì €ì¥ ê°œìˆ˜ (í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì • ê°€ëŠ¥, ê¸°ë³¸ê°’: 50,000ê°œ)
MAX_STORED_ITEMS = int(os.getenv("MAX_STORED_ITEMS", "50000"))

# ìµœëŒ€ ì»¨í…Œì´ë„ˆ ë©”ëª¨ë¦¬ ìš©ëŸ‰ (GB, í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì • ê°€ëŠ¥, ê¸°ë³¸ê°’: 7.5GB)
# Docker ì»¨í…Œì´ë„ˆ ë©”ëª¨ë¦¬ ì œí•œì— ë§ì¶° ì„¤ì •
MAX_CONTAINER_MEMORY_GB = float(os.getenv("MAX_CONTAINER_MEMORY_GB", "7.5"))

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì„ê³„ê°’ (ì´ ë¹„ìœ¨ì„ ì´ˆê³¼í•˜ë©´ ì •ë¦¬ ì‹œì‘, ê¸°ë³¸ê°’: 80%)
MEMORY_USAGE_THRESHOLD = float(os.getenv("MEMORY_USAGE_THRESHOLD", "0.8"))

# ìµœëŒ€ ë””ìŠ¤í¬ ìš©ëŸ‰ (GB, í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì • ê°€ëŠ¥, ê¸°ë³¸ê°’: 800GB)
# Docker ì œí•œì´ 1006.85GBì´ë¯€ë¡œ 80%ì¸ 800GBë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
MAX_DISK_SIZE_GB = float(os.getenv("MAX_DISK_SIZE_GB", "800"))

# ë””ìŠ¤í¬ ìš©ëŸ‰ ì„ê³„ê°’ (ì´ ë¹„ìœ¨ì„ ì´ˆê³¼í•˜ë©´ ì •ë¦¬ ì‹œì‘, ê¸°ë³¸ê°’: 80%)
DISK_USAGE_THRESHOLD = float(os.getenv("DISK_USAGE_THRESHOLD", "0.8"))

def check_event_loop():
    """
    Windowsì—ì„œ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    if sys.platform == 'win32':
        try:
            loop = asyncio.get_running_loop()
            if isinstance(loop, asyncio.ProactorEventLoop):
                logger.error("âŒ ProactorEventLoopê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤! psycopgì™€ í˜¸í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                raise RuntimeError(
                    "ProactorEventLoopëŠ” psycopgì™€ í˜¸í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
                    "SelectorEventLoopë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
                )
        except RuntimeError:
            # ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ë¬´ì‹œ
            pass


async def get_container_memory_usage_gb() -> Optional[float]:
    """
    Docker ì»¨í…Œì´ë„ˆì˜ í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ GB ë‹¨ìœ„ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (GB), ì‹¤íŒ¨ ì‹œ None
    """
    try:
        # í”„ë¡œì„¸ìŠ¤ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ (RSS - Resident Set Size)
        process = psutil.Process()
        memory_bytes = process.memory_info().rss
        
        # GBë¡œ ë³€í™˜
        memory_gb = memory_bytes / (1024 ** 3)
        return memory_gb
    except Exception as e:
        logger.error(f"âŒ ì»¨í…Œì´ë„ˆ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {type(e).__name__} - {e}")
        return None


async def get_database_size_gb(session: AsyncSession) -> float:
    """
    ë°ì´í„°ë² ì´ìŠ¤ì˜ í˜„ì¬ í¬ê¸°ë¥¼ GB ë‹¨ìœ„ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        session: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
    
    Returns:
        ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° (GB)
    """
    try:
        # PostgreSQLì˜ pg_database_size í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° í™•ì¸
        # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ì„ ê°€ì ¸ì˜´
        db_name_result = await session.execute(text("SELECT current_database()"))
        db_name = db_name_result.scalar()
        
        # ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° ì¡°íšŒ (ë°”ì´íŠ¸ ë‹¨ìœ„)
        size_result = await session.execute(
            text(f"SELECT pg_database_size('{db_name}')")
        )
        size_bytes = size_result.scalar() or 0
        
        # GBë¡œ ë³€í™˜
        size_gb = size_bytes / (1024 ** 3)
        return size_gb
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° ì¡°íšŒ ì‹¤íŒ¨: {type(e).__name__} - {e}")
        return 0.0


async def cleanup_by_memory_usage(session: AsyncSession, max_memory_gb: float = MAX_CONTAINER_MEMORY_GB, threshold: float = MEMORY_USAGE_THRESHOLD) -> int:
    """
    ì»¨í…Œì´ë„ˆ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ë©´ ê°€ì¥ ì˜¤ë˜ëœ ë°ì´í„°ë¶€í„° ì‚­ì œí•©ë‹ˆë‹¤.
    
    Args:
        session: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        max_memory_gb: ìµœëŒ€ ì»¨í…Œì´ë„ˆ ë©”ëª¨ë¦¬ ìš©ëŸ‰ (GB)
        threshold: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì„ê³„ê°’ ë¹„ìœ¨ (0.0 ~ 1.0)
    
    Returns:
        ì‚­ì œëœ ì•„ì´í…œ ìˆ˜
    """
    try:
        # í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        current_memory_gb = await get_container_memory_usage_gb()
        
        if current_memory_gb is None:
            logger.warning("âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•  ìˆ˜ ì—†ì–´ ë©”ëª¨ë¦¬ ê¸°ë°˜ ì •ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return 0
        
        threshold_memory_gb = max_memory_gb * threshold
        
        logger.info(f"ğŸ’¾ í˜„ì¬ ì»¨í…Œì´ë„ˆ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {current_memory_gb:.2f}GB / ìµœëŒ€ {max_memory_gb:.2f}GB (ì„ê³„ê°’: {threshold_memory_gb:.2f}GB)")
        
        if current_memory_gb < threshold_memory_gb:
            return 0
        
        # ì„ê³„ê°’ì„ ì´ˆê³¼í•œ ê²½ìš°, ëª©í‘œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê¹Œì§€ ì‚­ì œ
        # ëª©í‘œ ë©”ëª¨ë¦¬ëŠ” ì„ê³„ê°’ì˜ 70%ë¡œ ì„¤ì • (ì—¬ìœ  ê³µê°„ í™•ë³´)
        target_memory_gb = max_memory_gb * 0.7
        
        deleted_total = 0
        batch_size = 1000  # í•œ ë²ˆì— ì‚­ì œí•  ê°œìˆ˜
        max_iterations = 50  # ë¬´í•œ ë£¨í”„ ë°©ì§€
        
        iteration = 0
        while current_memory_gb > target_memory_gb and iteration < max_iterations:
            iteration += 1
            
            # ê°€ì¥ ì˜¤ë˜ëœ ì•„ì´í…œë“¤ì˜ ID ì¡°íšŒ
            old_items_query = (
                select(CollectedItem.id)
                .order_by(CollectedItem.collected_at.asc())
                .limit(batch_size)
            )
            old_items_result = await session.execute(old_items_query)
            old_item_ids = [row[0] for row in old_items_result.all()]
            
            if not old_item_ids:
                break
            
            # ì‚­ì œ ì‹¤í–‰
            delete_result = await session.execute(
                delete(CollectedItem).where(CollectedItem.id.in_(old_item_ids))
            )
            deleted_count = delete_result.rowcount
            deleted_total += deleted_count
            
            # ì„¸ì…˜ ì»¤ë°‹
            await session.commit()
            
            # ì‚­ì œ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë‹¤ì‹œ í™•ì¸
            current_memory_gb = await get_container_memory_usage_gb()
            if current_memory_gb is None:
                logger.warning("âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ ì‹¤íŒ¨, ì •ë¦¬ ì¤‘ë‹¨")
                break
            
            logger.info(f"ğŸ—‘ï¸ {deleted_count}ê°œ ì‚­ì œ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {current_memory_gb:.2f}GB (ëª©í‘œ: {target_memory_gb:.2f}GB)")
            
            # ë” ì´ìƒ ì‚­ì œí•  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
            if deleted_count == 0:
                break
        
        if deleted_total > 0:
            logger.info(f"ğŸ—‘ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ì •ë¦¬ ì™„ë£Œ: ì´ {deleted_total}ê°œ ì‚­ì œ (ìµœì¢… ë©”ëª¨ë¦¬: {current_memory_gb:.2f}GB)")
        
        return deleted_total
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ì •ë¦¬ ì‹¤íŒ¨: {type(e).__name__} - {e}")
        return 0


async def cleanup_by_disk_size(session: AsyncSession, max_size_gb: float = MAX_DISK_SIZE_GB, threshold: float = DISK_USAGE_THRESHOLD) -> int:
    """
    ë””ìŠ¤í¬ ìš©ëŸ‰ì´ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ë©´ ê°€ì¥ ì˜¤ë˜ëœ ë°ì´í„°ë¶€í„° ì‚­ì œí•©ë‹ˆë‹¤.
    
    Args:
        session: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        max_size_gb: ìµœëŒ€ ë””ìŠ¤í¬ ìš©ëŸ‰ (GB)
        threshold: ìš©ëŸ‰ ì„ê³„ê°’ ë¹„ìœ¨ (0.0 ~ 1.0)
    
    Returns:
        ì‚­ì œëœ ì•„ì´í…œ ìˆ˜
    """
    try:
        # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° í™•ì¸
        current_size_gb = await get_database_size_gb(session)
        threshold_size_gb = max_size_gb * threshold
        
        logger.info(f"ğŸ’¾ í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸°: {current_size_gb:.2f}GB / ìµœëŒ€ {max_size_gb:.2f}GB (ì„ê³„ê°’: {threshold_size_gb:.2f}GB)")
        
        if current_size_gb < threshold_size_gb:
            return 0
        
        # ì„ê³„ê°’ì„ ì´ˆê³¼í•œ ê²½ìš°, ëª©í‘œ í¬ê¸°ê¹Œì§€ ì‚­ì œ
        # ëª©í‘œ í¬ê¸°ëŠ” ì„ê³„ê°’ì˜ 70%ë¡œ ì„¤ì • (ì—¬ìœ  ê³µê°„ í™•ë³´)
        target_size_gb = max_size_gb * 0.7
        
        # ì‚­ì œí•  ë°ì´í„° ì–‘ ê³„ì‚° (ëŒ€ëµì ìœ¼ë¡œ)
        # ê° ì•„ì´í…œì˜ í‰ê·  í¬ê¸°ë¥¼ ì¶”ì •í•˜ì—¬ ì‚­ì œí•  ê°œìˆ˜ ê³„ì‚°
        # ì‹¤ì œë¡œëŠ” ë°˜ë³µì ìœ¼ë¡œ ì‚­ì œí•˜ë©´ì„œ í¬ê¸°ë¥¼ í™•ì¸
        deleted_total = 0
        batch_size = 1000  # í•œ ë²ˆì— ì‚­ì œí•  ê°œìˆ˜
        
        while current_size_gb > target_size_gb:
            # ê°€ì¥ ì˜¤ë˜ëœ ì•„ì´í…œë“¤ì˜ ID ì¡°íšŒ
            old_items_query = (
                select(CollectedItem.id)
                .order_by(CollectedItem.collected_at.asc())
                .limit(batch_size)
            )
            old_items_result = await session.execute(old_items_query)
            old_item_ids = [row[0] for row in old_items_result.all()]
            
            if not old_item_ids:
                break
            
            # ì‚­ì œ ì‹¤í–‰
            delete_result = await session.execute(
                delete(CollectedItem).where(CollectedItem.id.in_(old_item_ids))
            )
            deleted_count = delete_result.rowcount
            deleted_total += deleted_count
            
            # ì„¸ì…˜ ì»¤ë°‹
            await session.commit()
            
            # ì‚­ì œ í›„ í¬ê¸° ë‹¤ì‹œ í™•ì¸
            current_size_gb = await get_database_size_gb(session)
            logger.info(f"ğŸ—‘ï¸ {deleted_count}ê°œ ì‚­ì œ í›„ ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸°: {current_size_gb:.2f}GB (ëª©í‘œ: {target_size_gb:.2f}GB)")
            
            # ë” ì´ìƒ ì‚­ì œí•  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
            if deleted_count == 0:
                break
        
        if deleted_total > 0:
            logger.info(f"ğŸ—‘ï¸ ë””ìŠ¤í¬ ìš©ëŸ‰ ì •ë¦¬ ì™„ë£Œ: ì´ {deleted_total}ê°œ ì‚­ì œ (ìµœì¢… í¬ê¸°: {current_size_gb:.2f}GB)")
        
        return deleted_total
    except Exception as e:
        logger.error(f"âŒ ë””ìŠ¤í¬ ìš©ëŸ‰ ê¸°ë°˜ ì •ë¦¬ ì‹¤íŒ¨: {type(e).__name__} - {e}")
        return 0


async def cleanup_old_items(session: AsyncSession, max_items: int = MAX_STORED_ITEMS) -> int:
    """
    ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ ì•„ì´í…œì´ ìµœëŒ€ ê°œìˆ˜ë¥¼ ì´ˆê³¼í•˜ë©´ ê°€ì¥ ì˜¤ë˜ëœ ë°ì´í„°ë¶€í„° ì‚­ì œí•©ë‹ˆë‹¤.
    
    Args:
        session: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        max_items: ìµœëŒ€ ì €ì¥ ê°œìˆ˜
    
    Returns:
        ì‚­ì œëœ ì•„ì´í…œ ìˆ˜
    """
    try:
        # í˜„ì¬ ì €ì¥ëœ ì•„ì´í…œ ê°œìˆ˜ í™•ì¸
        count_result = await session.execute(select(func.count(CollectedItem.id)))
        current_count = count_result.scalar() or 0
        
        if current_count <= max_items:
            return 0
        
        # ì‚­ì œí•  ê°œìˆ˜ ê³„ì‚°
        items_to_delete = current_count - max_items
        
        # ê°€ì¥ ì˜¤ë˜ëœ ì•„ì´í…œë¶€í„° ì‚­ì œ (collected_at ê¸°ì¤€ ì˜¤ë¦„ì°¨ìˆœ)
        # ë¨¼ì € ì‚­ì œí•  ì•„ì´í…œë“¤ì˜ IDë¥¼ ì¡°íšŒ
        old_items_query = (
            select(CollectedItem.id)
            .order_by(CollectedItem.collected_at.asc())
            .limit(items_to_delete)
        )
        old_items_result = await session.execute(old_items_query)
        old_item_ids = [row[0] for row in old_items_result.all()]
        
        if old_item_ids:
            # ID ëª©ë¡ì„ ì‚¬ìš©í•˜ì—¬ ì‚­ì œ
            delete_result = await session.execute(
                delete(CollectedItem).where(CollectedItem.id.in_(old_item_ids))
            )
            deleted_count = delete_result.rowcount
        else:
            deleted_count = 0
        
        if deleted_count > 0:
            logger.info(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ë°ì´í„° {deleted_count}ê°œ ì‚­ì œ (ìµœëŒ€ ì €ì¥ ê°œìˆ˜: {max_items}ê°œ ìœ ì§€)")
        
        return deleted_count
    except Exception as e:
        logger.error(f"âŒ ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {type(e).__name__} - {e}")
        return 0


async def save_collected_items(items: List[Dict[str, Any]], source_type: str) -> int:
    """
    ìˆ˜ì§‘ëœ ì•„ì´í…œë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        items: ìˆ˜ì§‘ëœ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸
        source_type: ì†ŒìŠ¤ íƒ€ì… (ì˜ˆ: "reddit", "news", "github", "youtube")
    
    Returns:
        ì €ì¥ëœ ì•„ì´í…œ ìˆ˜
    """
    # ì´ë²¤íŠ¸ ë£¨í”„ í™•ì¸
    check_event_loop()
    
    if not items:
        return 0
    
    saved_count = 0
    async with AsyncSessionLocal() as session:
        try:
            # ì €ì¥ ì „ì— ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ì •ë¦¬ (ìµœìš°ì„ ìˆœìœ„)
            deleted_by_memory = await cleanup_by_memory_usage(session, MAX_CONTAINER_MEMORY_GB, MEMORY_USAGE_THRESHOLD)
            
            # ì €ì¥ ì „ì— ë””ìŠ¤í¬ ìš©ëŸ‰ ê¸°ë°˜ ì •ë¦¬ (ìš°ì„ ìˆœìœ„ 2)
            deleted_by_size = await cleanup_by_disk_size(session, MAX_DISK_SIZE_GB, DISK_USAGE_THRESHOLD)
            
            # ì €ì¥ ì „ì— ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬ (ìµœëŒ€ ê°œìˆ˜ ì´ˆê³¼ ì‹œ, ìš°ì„ ìˆœìœ„ 3)
            deleted_by_count = await cleanup_old_items(session, MAX_STORED_ITEMS)
            
            for item in items:
                # ì¤‘ë³µ ì²´í¬ ì—†ì´ ëª¨ë“  ë°ì´í„° ì €ì¥
                url = item.get("url", "")
                title = item.get("title", "")
                
                # collected_at íŒŒì‹±
                collected_at_str = item.get("collected_at")
                if collected_at_str:
                    try:
                        if isinstance(collected_at_str, str):
                            collected_at = datetime.fromisoformat(collected_at_str.replace('Z', '+00:00'))
                        else:
                            collected_at = datetime.now()
                    except:
                        collected_at = datetime.now()
                else:
                    collected_at = datetime.now()
                
                # CollectedItem ìƒì„±
                collected_item = CollectedItem(
                    source=item.get("source", "Unknown"),
                    source_type=source_type,
                    title=title,
                    content=item.get("description") or item.get("content", ""),
                    url=url,
                    extra_data={
                        "upvotes": item.get("upvotes"),
                        "likes": item.get("likes"),
                        "views": item.get("views"),
                        "comments": item.get("comments"),
                        "retweets": item.get("retweets"),
                        "stars": item.get("stars"),
                        "subreddit": item.get("subreddit"),
                        "channel": item.get("channel"),
                        "published": item.get("published_at") or item.get("published"),
                        **{k: v for k, v in item.items() if k not in [
                            "source", "title", "description", "content", "url",
                            "upvotes", "likes", "views", "comments", "retweets",
                            "stars", "subreddit", "channel", "published_at", "published",
                            "collected_at"
                        ]}
                    },
                    collected_at=collected_at
                )
                
                session.add(collected_item)
                saved_count += 1
            
            await session.commit()
            
            # ì €ì¥ í›„ í˜„ì¬ ë°ì´í„° ê°œìˆ˜, ë””ìŠ¤í¬ ìš©ëŸ‰, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            count_result = await session.execute(select(func.count(CollectedItem.id)))
            total_count = count_result.scalar() or 0
            db_size_gb = await get_database_size_gb(session)
            memory_gb = await get_container_memory_usage_gb()
            memory_info = f", ë©”ëª¨ë¦¬: {memory_gb:.2f}GB" if memory_gb is not None else ""
            
            logger.info(f"ğŸ’¾ {source_type} ë°ì´í„° ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ ì €ì¥ (ì´ {len(items)}ê°œ ì¤‘, í˜„ì¬ DB ì´ {total_count}ê°œ, í¬ê¸°: {db_size_gb:.2f}GB{memory_info})")
            
        except Exception as e:
            await session.rollback()
            logger.error(f"âŒ {source_type} ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {type(e).__name__} - {e}")
            raise
    
    return saved_count


async def save_all_collected_data(collected_data: Dict[str, List[Dict[str, Any]]]) -> Dict[str, int]:
    """
    ëª¨ë“  ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        collected_data: ì†ŒìŠ¤ë³„ë¡œ ë¶„ë¥˜ëœ ìˆ˜ì§‘ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    
    Returns:
        ì†ŒìŠ¤ë³„ ì €ì¥ëœ ì•„ì´í…œ ìˆ˜ ë”•ì…”ë„ˆë¦¬
    """
    logger.info("ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹œì‘...")
    
    save_results = {}
    
    # ê° ì†ŒìŠ¤ íƒ€ì…ë³„ë¡œ ì €ì¥
    source_type_mapping = {
        "reddit": "reddit",
        "reddit_subreddits": "reddit",
        "news": "news",
        "github": "github",
        "youtube": "youtube"
    }
    
    for source_key, items in collected_data.items():
        if items:
            source_type = source_type_mapping.get(source_key, source_key)
            try:
                saved_count = await save_collected_items(items, source_type)
                save_results[source_key] = saved_count
            except Exception as e:
                logger.error(f"âŒ {source_key} ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                save_results[source_key] = 0
    
    total_saved = sum(save_results.values())
    logger.info(f"ğŸ’¾ ì „ì²´ ì €ì¥ ì™„ë£Œ! ì´ {total_saved}ê°œ ì•„ì´í…œ ì €ì¥ë¨")
    
    return save_results


async def get_recent_items(source_type: str = None, limit: int = 10) -> List[CollectedItem]:
    """
    ìµœê·¼ ìˆ˜ì§‘ëœ ì•„ì´í…œì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    Args:
        source_type: ì†ŒìŠ¤ íƒ€ì… í•„í„° (Noneì´ë©´ ì „ì²´)
        limit: ì¡°íšŒí•  ìµœëŒ€ ê°œìˆ˜
    
    Returns:
        CollectedItem ë¦¬ìŠ¤íŠ¸
    """
    # ì´ë²¤íŠ¸ ë£¨í”„ í™•ì¸
    check_event_loop()
    
    async with AsyncSessionLocal() as session:
        try:
            query = select(CollectedItem).order_by(CollectedItem.collected_at.desc())
            
            if source_type:
                query = query.where(CollectedItem.source_type == source_type)
            
            query = query.limit(limit)
            
            result = await session.execute(query)
            return list(result.scalars().all())
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {type(e).__name__} - {e}")
            return []

