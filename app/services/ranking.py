"""
ì´ìŠˆ ë­í‚¹ ì‹œìŠ¤í…œ
ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì£¼ìš” ì´ìŠˆë¥¼ ë­í‚¹í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
"""
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, desc, and_
from app.core.database import AsyncSessionLocal
from app.core.models import AnalysisResult, IssueRanking, CollectedItem

logger = logging.getLogger("hourly_pulse")


async def calculate_item_interest_score(item: CollectedItem) -> int:
    """
    ë‹¨ì¼ ì•„ì´í…œì˜ ê´€ì‹¬ë„ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        item: CollectedItem ê°ì²´
    
    Returns:
        ê´€ì‹¬ë„ ì ìˆ˜ (ì¶”ì • ì¡°íšŒìˆ˜)
    """
    if not item.extra_data:
        return 100  # ê¸°ë³¸ê°’
    
    extra = item.extra_data if isinstance(item.extra_data, dict) else {}
    source_type = item.source_type or 'unknown'
    
    estimated_views = 0
    
    try:
        if source_type == 'youtube':
            # YouTube: ì‹¤ì œ ì¡°íšŒìˆ˜ ì‚¬ìš© (ê°€ì¥ ì •í™•)
            estimated_views = int(extra.get('views', 0) or 0)
            
        elif source_type == 'reddit':
            # Reddit: upvotesì™€ commentsë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¡°íšŒìˆ˜ ì¶”ì •
            # ì‹¤ì œ Redditì˜ view:upvote ë¹„ìœ¨ì€ ì•½ 10:1 ~ 50:1
            # ê°œì„ : 80ë°° â†’ 15ë°°ë¡œ ì¡°ì • (ë” í˜„ì‹¤ì ì¸ ì¶”ì •)
            upvotes = max(0, int(extra.get('upvotes', 0) or 0))
            comments = max(0, int(extra.get('comments', 0) or 0))
            estimated_views = (upvotes * 15) + (comments * 5)
            
        elif source_type == 'github':
            # GitHub: stars, forks, watchersë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¡°íšŒìˆ˜ ì¶”ì •
            # StarsëŠ” "ì¢‹ì•„ìš”" ê°œë…ì´ë¯€ë¡œ viewsì™€ ì§ì ‘ì ì¸ ìƒê´€ê´€ê³„ê°€ ë‚®ìŒ
            # ê°œì„ : 200ë°° â†’ 20ë°°ë¡œ ì¡°ì • (ë” í˜„ì‹¤ì ì¸ ì¶”ì •)
            stars = max(0, int(extra.get('stars', 0) or 0))
            forks = max(0, int(extra.get('forks', 0) or 0))
            watchers = max(0, int(extra.get('watchers', 0) or 0))
            # GitHub ì €ì¥ì†Œì˜ ì‹¤ì œ viewsëŠ” starsì˜ ì•½ 5~10ë°° ì •ë„
            estimated_views = (stars * 20) + (forks * 10) + (watchers * 3)
            
        elif source_type == 'news':
            # News: AI ì¶”ì • ë˜ëŠ” íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©
            # ì£¼ì˜: AI ì¶”ì •ì€ ì„±ëŠ¥ ë¬¸ì œë¡œ ì¸í•´ ì„ íƒì ìœ¼ë¡œë§Œ ì‚¬ìš©
            # í˜„ì¬ëŠ” ê°œì„ ëœ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš© (í•„ìš”ì‹œ AI ì¶”ì • ì¶”ê°€ ê°€ëŠ¥)
            
            comments = max(0, int(extra.get('comments', 0) or 0))
            if comments > 0:
                # ëŒ“ê¸€ì´ ìˆìœ¼ë©´ ëŒ“ê¸€ ìˆ˜ ê¸°ë°˜ ì¶”ì •
                estimated_views = comments * 50
            else:
                # ê°œì„ ëœ íœ´ë¦¬ìŠ¤í‹± ì ìˆ˜ ê³„ì‚°
                estimated_views = _calculate_news_heuristic_score(item)
                
        else:
            estimated_views = 100
        
        # ì—£ì§€ ì¼€ì´ìŠ¤ ì²˜ë¦¬
        # ìŒìˆ˜ ë°©ì§€
        estimated_views = max(0, estimated_views)
        # ë§¤ìš° í° ê°’ ì œí•œ (ì˜¤ë²„í”Œë¡œìš° ë°©ì§€, BigInteger ë²”ìœ„ ë‚´)
        estimated_views = min(estimated_views, 10_000_000_000)  # 100ì–µ ì œí•œ
        
    except (ValueError, TypeError) as e:
        logger.warning(f"âš ï¸ ê´€ì‹¬ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ (item_id={item.id}, source_type={source_type}): {e}")
        estimated_views = 100  # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’
    
    return int(estimated_views)


def _calculate_news_heuristic_score(item: CollectedItem) -> int:
    """
    News ì•„ì´í…œì˜ íœ´ë¦¬ìŠ¤í‹± ê´€ì‹¬ë„ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        item: CollectedItem ê°ì²´
    
    Returns:
        ì¶”ì • ì¡°íšŒìˆ˜
    """
    title = item.title or ""
    content = item.content or ""
    
    # 1. ì œëª© ê¸¸ì´ ì ìˆ˜ (ì ì ˆí•œ ê¸¸ì´ì˜ ì œëª©ì´ ë” ë†’ì€ ì ìˆ˜)
    title_length = len(title)
    if 20 <= title_length <= 100:
        length_score = 30  # ìµœì  ê¸¸ì´
    elif 10 <= title_length < 20 or 100 < title_length <= 150:
        length_score = 20  # ë³´í†µ ê¸¸ì´
    else:
        length_score = 10  # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ì œëª©
    
    # 2. ì¤‘ìš” í‚¤ì›Œë“œ ì ìˆ˜ (ì¤‘ë³µ ì œê±°)
    important_keywords = ['breaking', 'urgent', 'major', 'crisis', 'alert', 'important']
    keyword_score = sum(15 for kw in important_keywords if kw.lower() in title.lower())
    
    # 3. ë‚´ìš© ê¸¸ì´ ì ìˆ˜ (ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¶”ê°€ ì ìˆ˜)
    content_length = len(content) if content else 0
    content_score = min(content_length / 100, 20)  # ìµœëŒ€ 20ì 
    
    # 4. ê¸°ë³¸ ì ìˆ˜
    base_score = 100
    
    estimated_views = base_score + int(length_score) + keyword_score + int(content_score)
    return estimated_views


async def calculate_issue_rankings(hours: int = 1) -> List[Dict[str, Any]]:
    """
    ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ìŠˆ ë­í‚¹ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        hours: ë¶„ì„í•  ìµœê·¼ ì‹œê°„ ë²”ìœ„
    
    Returns:
        ë­í‚¹ëœ ì´ìŠˆ ë¦¬ìŠ¤íŠ¸
    """
    logger.info("ğŸ“Š ì´ìŠˆ ë­í‚¹ ê³„ì‚° ì‹œì‘ (5ë¶„ë§ˆë‹¤ ìˆ˜ì§‘ëœ ë°ì´í„° ê¸°ë°˜)...")
    
    async with AsyncSessionLocal() as session:
        try:
            from datetime import timezone
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours)
            
            # ìµœê·¼ ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            analysis_query = select(AnalysisResult).where(
                AnalysisResult.analyzed_at >= cutoff_time
            ).order_by(desc(AnalysisResult.analyzed_at))
            
            analysis_result = await session.execute(analysis_query)
            analysis_results = list(analysis_result.scalars().all())
            
            if not analysis_results:
                logger.warning("âš ï¸ ë¶„ì„ ê²°ê³¼ê°€ ì—†ì–´ ë­í‚¹ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            logger.info(f"ğŸ“Š ë¶„ì„ ê²°ê³¼ {len(analysis_results)}ê°œë¥¼ ë‚´ìš© ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜ ì¤‘...")
            
            # ì´ìŠˆë³„ë¡œ ê·¸ë£¹í™” ë° ì ìˆ˜ ê³„ì‚° (ë‚´ìš© ê¸°ë°˜)
            # ë‹¨ìˆœ í† í”½ëª…ì´ ì•„ë‹Œ, ì‹¤ì œ ì´ìŠˆ ë‚´ìš©(what, why_now, context)ì„ ê¸°ë°˜ìœ¼ë¡œ ê·¸ë£¹í™”
            issue_scores = {}
            
            for result in analysis_results:
                # ì´ìŠˆ ì‹ë³„: í† í”½ëª… + ë‚´ìš© ê¸°ë°˜
                topic = result.topic
                what = result.what or ''
                why_now = result.why_now or ''
                context = result.context or ''
                summary = result.summary or ''
                
                # ì´ìŠˆì˜ ê³ ìœ  ì‹ë³„ì ìƒì„± (ë‚´ìš© ê¸°ë°˜)
                # why_nowê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ what, ì—†ìœ¼ë©´ summary
                issue_content = why_now if why_now else (what if what else summary)
                
                # ì´ìŠˆ í‚¤: í† í”½ëª… + ì£¼ìš” ë‚´ìš© ê¸°ë°˜ ê·¸ë£¹í™”
                # ìœ ì‚¬í•œ ë‚´ìš©ì„ ê°™ì€ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ê¸° ìœ„í•´ í† í”½ëª…ê³¼ ì£¼ìš” í‚¤ì›Œë“œ ì¡°í•© ì‚¬ìš©
                # í† í”½ëª…ì„ ì •ê·œí™” (ì†Œë¬¸ì, ê³µë°± ì œê±°)í•˜ì—¬ ìœ ì‚¬í•œ í† í”½ì„ ë¬¶ìŒ
                normalized_topic = topic.lower().strip() if topic else ''
                # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ ë‹¨ì–´ë§Œ)
                topic_words = [w for w in normalized_topic.split() if len(w) > 2]
                # í† í”½ëª…ì˜ í•µì‹¬ í‚¤ì›Œë“œë¡œ ê·¸ë£¹í™” (ìµœëŒ€ 3ê°œ í‚¤ì›Œë“œ ì‚¬ìš©)
                if topic_words:
                    # ê°€ì¥ ê¸´ í‚¤ì›Œë“œ 3ê°œë¥¼ ì„ íƒí•˜ì—¬ ê·¸ë£¹ í‚¤ ìƒì„±
                    sorted_words = sorted(topic_words, key=len, reverse=True)[:3]
                    issue_key = ' '.join(sorted_words)
                else:
                    issue_key = normalized_topic if normalized_topic else 'unknown'
                
                if issue_key not in issue_scores:
                    issue_scores[issue_key] = {
                        'topic': topic,
                        'what': what,
                        'why_now': why_now,
                        'context': context,
                        'summary': summary,
                        'importance_scores': [],
                        'source_counts': [],
                        'mention_counts': [],
                        'sentiments': [],
                        'collected_item_ids': set(),
                        'analysis_ids': [],
                        'content_quality_score': 0.0,  # ë‚´ìš© í’ˆì§ˆ ì ìˆ˜
                        'temporal_relevance_score': 0.0  # ì‹œì  ê´€ë ¨ì„± ì ìˆ˜
                    }
                
                # ì ìˆ˜ ìˆ˜ì§‘
                issue_scores[issue_key]['importance_scores'].append(result.importance_score or 0.0)
                issue_scores[issue_key]['source_counts'].append(result.source_count or 0)
                issue_scores[issue_key]['sentiments'].append(result.sentiment or 'neutral')
                issue_scores[issue_key]['analysis_ids'].append(result.id)
                
                # ë‚´ìš© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (what, why_now, contextê°€ ëª¨ë‘ ìˆìœ¼ë©´ ë†’ì€ ì ìˆ˜)
                content_score = 0.0
                if what:
                    content_score += 0.3
                if why_now:
                    content_score += 0.5  # why_nowê°€ ê°€ì¥ ì¤‘ìš”
                if context:
                    content_score += 0.2
                issue_scores[issue_key]['content_quality_score'] = max(
                    issue_scores[issue_key]['content_quality_score'],
                    content_score
                )
                
                # ì‹œì  ê´€ë ¨ì„± ì ìˆ˜ (why_nowê°€ ìˆìœ¼ë©´ ë†’ì€ ì ìˆ˜)
                if why_now:
                    issue_scores[issue_key]['temporal_relevance_score'] = 1.0
                elif what or context:
                    issue_scores[issue_key]['temporal_relevance_score'] = 0.5
                
                # ê°€ì¥ ìƒì„¸í•œ ë‚´ìš©ìœ¼ë¡œ ì—…ë°ì´íŠ¸ (why_now ìš°ì„ )
                if why_now and not issue_scores[issue_key]['why_now']:
                    issue_scores[issue_key]['why_now'] = why_now
                if what and not issue_scores[issue_key]['what']:
                    issue_scores[issue_key]['what'] = what
                if context and not issue_scores[issue_key]['context']:
                    issue_scores[issue_key]['context'] = context
                
                # ê´€ë ¨ ì•„ì´í…œ ID ìˆ˜ì§‘
                if result.collected_item_ids:
                    issue_scores[issue_key]['collected_item_ids'].update(result.collected_item_ids)
            
            # ê° ì´ìŠˆì˜ ì¢…í•© ì ìˆ˜ ê³„ì‚° (ë‚´ìš© ê¸°ë°˜ ë¹„êµ ë¶„ì„)
            ranked_issues = []
            
            for issue_key, data in issue_scores.items():
                # í‰ê·  ì¤‘ìš”ë„ ì ìˆ˜
                avg_importance = sum(data['importance_scores']) / len(data['importance_scores']) if data['importance_scores'] else 0.0
                
                # ìµœëŒ€ ì†ŒìŠ¤ ìˆ˜
                max_sources = max(data['source_counts']) if data['source_counts'] else 0
                
                # ì¶œì²˜ ì •ë³´ ì´ˆê¸°í™”
                top_source_types = []
                top_source_names = []
                
                # ê´€ì‹¬ë„ ê³„ì‚°: ìµœê·¼ 5ë¶„ ë™ì•ˆ ìˆ˜ì§‘ëœ ìœ ì‚¬í•œ ë‚´ìš©ì˜ ëª¨ë“  ì•„ì´í…œ ê´€ì‹¬ë„ í•©ì‚°
                # 5ë¶„ë§ˆë‹¤ ìˆ˜ì§‘ëœ ë°ì´í„°ë¡œ ë‚´ìš© ê·¸ë£¹ì˜ ê´€ì‹¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ìˆœìœ„ë¥¼ ë§¤ê¹€
                interest_score = 0
                mention_count = 0
                from datetime import timezone
                now = datetime.now(timezone.utc)
                # ìµœê·¼ 5ë¶„ ê¸°ì¤€ ì‹œê°„ (5ë¶„ë§ˆë‹¤ ìˆ˜ì§‘ëœ ë°ì´í„°ë§Œ ì‚¬ìš©)
                five_minutes_ago = now - timedelta(minutes=5)
                logger.debug(f"ğŸ“Š [{data['topic']}] ìµœê·¼ 5ë¶„ ë°ì´í„° ê¸°ì¤€ ì‹œê°„: {five_minutes_ago.isoformat()} ~ {now.isoformat()}")
                
                # í† í”½ëª…ê³¼ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ
                topic = data['topic'].lower() if data['topic'] else ''
                # í† í”½ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬)
                topic_keywords = [kw.strip() for kw in topic.split() if len(kw.strip()) > 2]
                if not topic_keywords:
                    topic_keywords = [topic] if topic else []
                
                # ë³€ìˆ˜ ì´ˆê¸°í™”
                items = []
                additional_items = []
                
                # ë°©ë²• 1: collected_item_idsì— í¬í•¨ëœ ì•„ì´í…œ ì¤‘ ìµœê·¼ 5ë¶„ ë‚´ ì•„ì´í…œ
                if data['collected_item_ids']:
                    item_ids = list(data['collected_item_ids'])
                    # ê´€ë ¨ ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸° (ìµœê·¼ 5ë¶„ í•„í„° ì ìš©)
                    items_query = select(CollectedItem).where(
                        CollectedItem.id.in_(item_ids[:200]),  # ìµœëŒ€ 200ê°œê¹Œì§€ í™•ì¸
                        CollectedItem.collected_at >= five_minutes_ago  # ìµœê·¼ 5ë¶„ ë‚´ ì•„ì´í…œë§Œ
                    )
                    items_result = await session.execute(items_query)
                    items = list(items_result.scalars().all())
                    
                    # collected_item_idsì— í¬í•¨ëœ ì•„ì´í…œì€ ëª¨ë‘ ê´€ë ¨ ì•„ì´í…œìœ¼ë¡œ ê°„ì£¼
                    for item in items:
                        # calculate_item_interest_score í•¨ìˆ˜ ì‚¬ìš©
                        item_score = await calculate_item_interest_score(item)
                        interest_score += item_score
                        mention_count += 1
                    
                    logger.info(f"ğŸ“Š [{data['topic']}] collected_item_ids ê¸°ë°˜ ìµœê·¼ 5ë¶„ ë‚´ ì•„ì´í…œ: {len(items)}ê°œ, ê´€ì‹¬ë„ í•©ê³„: {interest_score}")
                
                # ë°©ë²• 2: collected_item_ids ì™¸ì—ë„ í† í”½ í‚¤ì›Œë“œë¡œ ìµœê·¼ 5ë¶„ ë‚´ ì¶”ê°€ ê²€ìƒ‰
                # (ë” ë§ì€ ìœ ì‚¬í•œ ë‚´ìš©ì˜ ì•„ì´í…œì„ ì°¾ê¸° ìœ„í•´)
                if topic_keywords:
                    from sqlalchemy import or_
                    # ìµœê·¼ 5ë¶„ ë‚´ ëª¨ë“  ì•„ì´í…œ ì¤‘ í† í”½ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì•„ì´í…œ ê²€ìƒ‰
                    keyword_items_query = select(CollectedItem).where(
                        CollectedItem.collected_at >= five_minutes_ago
                    )
                    
                    # ì œëª©ì´ë‚˜ ë‚´ìš©ì— í† í”½ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì•„ì´í…œ ì°¾ê¸°
                    title_conditions = [CollectedItem.title.ilike(f"%{kw}%") for kw in topic_keywords]
                    content_conditions = [CollectedItem.content.ilike(f"%{kw}%") for kw in topic_keywords]
                    keyword_items_query = keyword_items_query.where(
                        or_(*title_conditions, *content_conditions)
                    )
                    
                    keyword_items_result = await session.execute(keyword_items_query)
                    keyword_items = list(keyword_items_result.scalars().all())
                    
                    # collected_item_idsì— í¬í•¨ë˜ì§€ ì•Šì€ ì•„ì´í…œë§Œ ì¶”ê°€
                    existing_item_ids = {item.id for item in items}
                    additional_items = [item for item in keyword_items if item.id not in existing_item_ids]
                    
                    additional_interest = 0
                    for item in additional_items:
                        # calculate_item_interest_score í•¨ìˆ˜ ì‚¬ìš©
                        item_score = await calculate_item_interest_score(item)
                        interest_score += item_score
                        additional_interest += item_score
                        mention_count += 1
                    
                    if additional_items:
                        logger.info(f"ğŸ“Š [{data['topic']}] í† í”½ í‚¤ì›Œë“œë¡œ ì¶”ê°€ ë°œê²¬: {len(additional_items)}ê°œ, ì¶”ê°€ ê´€ì‹¬ë„: {additional_interest}")
                
                logger.info(f"ğŸ“Š [{data['topic']}] ìµœì¢… ê´€ì‹¬ë„ í•©ê³„: {interest_score} (ì´ {mention_count}ê°œ ì•„ì´í…œ)")
                
                # ì†ŒìŠ¤ ë‹¤ì–‘ì„± ê³„ì‚° (ê³ ìœ  ì†ŒìŠ¤ íƒ€ì… ìˆ˜)
                source_diversity = 0
                top_source_types = []
                top_source_names = []
                
                # ìµœê·¼ 5ë¶„ ë‚´ ì•„ì´í…œë“¤ì˜ ì†ŒìŠ¤ ì •ë³´ ìˆ˜ì§‘
                all_recent_items = items + additional_items
                if all_recent_items:
                    # ìµœê·¼ 5ë¶„ ë‚´ ì•„ì´í…œë“¤ì˜ ì†ŒìŠ¤ íƒ€ì… í™•ì¸
                    source_type_set = {item.source_type for item in all_recent_items if item.source_type}
                    source_diversity = len(source_type_set)
                    
                    # ì£¼ìš” ì†ŒìŠ¤ íƒ€ì… ë° ì†ŒìŠ¤ ì´ë¦„ ìˆ˜ì§‘ (ì¶œì²˜ ì •ë³´)
                    source_type_counts = {}
                    source_name_counts = {}
                    
                    for item in all_recent_items[:100]:  # ìµœëŒ€ 100ê°œ
                        source_type = item.source_type or 'unknown'
                        source_name = item.source or 'Unknown'
                        source_type_counts[source_type] = source_type_counts.get(source_type, 0) + 1
                        source_name_counts[source_name] = source_name_counts.get(source_name, 0) + 1
                    
                    # ìƒìœ„ ì†ŒìŠ¤ íƒ€ì… ë° ì†ŒìŠ¤ ì´ë¦„ (ë¹ˆë„ìˆœ)
                    top_source_types = sorted(source_type_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                    top_source_names = sorted(source_name_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                elif data['collected_item_ids']:
                    # ì•„ì´í…œì´ ì—†ìœ¼ë©´ collected_item_idsë¡œ ì†ŒìŠ¤ ì •ë³´ ì¡°íšŒ
                    all_item_ids = list(data['collected_item_ids'])[:200]
                    if all_item_ids:
                        source_types_query = select(CollectedItem.source_type).where(
                            CollectedItem.id.in_(all_item_ids)
                        ).distinct()
                        source_types_result = await session.execute(source_types_query)
                        source_diversity = len(list(source_types_result.scalars().all()))
                        
                        all_items_query = select(CollectedItem.source_type, CollectedItem.source).where(
                            CollectedItem.id.in_(all_item_ids)
                        )
                        all_items_result = await session.execute(all_items_query)
                        all_source_data = list(all_items_result.all())
                        
                        source_type_counts = {}
                        source_name_counts = {}
                        for source_type, source_name in all_source_data:
                            if source_type:
                                source_type_counts[source_type] = source_type_counts.get(source_type, 0) + 1
                            if source_name:
                                source_name_counts[source_name] = source_name_counts.get(source_name, 0) + 1
                        
                        top_source_types = sorted(source_type_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                        top_source_names = sorted(source_name_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                else:
                    # ìˆ˜ì§‘ëœ ì•„ì´í…œì´ ì—†ìœ¼ë©´ ë¶„ì„ ê²°ê³¼ ìˆ˜ë¥¼ ì‚¬ìš©
                    mention_count = len(data['analysis_ids'])
                    interest_score = mention_count  # ê¸°ë³¸ê°’: ì–¸ê¸‰ íšŸìˆ˜ì™€ ë™ì¼
                    source_diversity = 0
                    top_source_types = []
                    top_source_names = []
                
                # ê°ì • ë¶„ì„ (ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚œ ê°ì •)
                sentiment_counts = {}
                for sentiment in data['sentiments']:
                    sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1
                dominant_sentiment = max(sentiment_counts.items(), key=lambda x: x[1])[0] if sentiment_counts else 'neutral'
                
                # ë‚´ìš© ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
                # 1. ë‚´ìš© í’ˆì§ˆ ì ìˆ˜ (what, why_now, contextê°€ ëª¨ë‘ ìˆìœ¼ë©´ ë†’ì€ ì ìˆ˜)
                content_quality = data['content_quality_score']
                
                # 2. ì‹œì  ê´€ë ¨ì„± ì ìˆ˜ (why_nowê°€ ìˆìœ¼ë©´ í˜„ì¬ ì´ìŠˆë¡œ íŒë‹¨)
                temporal_relevance = data['temporal_relevance_score']
                
                # 3. ë‚´ìš©ì˜ ê¹Šì´ ì ìˆ˜ (why_nowì™€ contextê°€ ëª¨ë‘ ìˆìœ¼ë©´ ë†’ì€ ì ìˆ˜)
                depth_score = 0.0
                if data['why_now']:
                    depth_score += 0.6  # why_nowê°€ ê°€ì¥ ì¤‘ìš”
                if data['context']:
                    depth_score += 0.4
                
                # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ë‚´ìš© ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©)
                # ì¤‘ìš”ë„(25%) + ë‚´ìš© í’ˆì§ˆ(25%) + ì‹œì  ê´€ë ¨ì„±(20%) + ì–¸ê¸‰ íšŸìˆ˜(15%) + ì†ŒìŠ¤ ë‹¤ì–‘ì„±(10%) + ë‚´ìš© ê¹Šì´(5%)
                mention_score = min(mention_count / 10.0, 1.0)  # ìµœëŒ€ 10íšŒ = 1.0
                diversity_score = min(source_diversity / 5.0, 1.0)  # ìµœëŒ€ 5ê°œ ì†ŒìŠ¤ = 1.0
                source_score = min(max_sources / 10.0, 1.0)  # ìµœëŒ€ 10ê°œ ì†ŒìŠ¤ = 1.0
                
                final_score = (
                    avg_importance * 0.25 +
                    content_quality * 0.25 +
                    temporal_relevance * 0.20 +
                    mention_score * 0.15 +
                    diversity_score * 0.10 +
                    depth_score * 0.05
                )
                
                # ì´ìŠˆ ì„¤ëª… ìƒì„± (why_now ìš°ì„ , ì—†ìœ¼ë©´ what, ì—†ìœ¼ë©´ summary)
                issue_description = data['why_now'] if data['why_now'] else (
                    data['what'] if data['what'] else data['summary']
                )
                
                ranked_issues.append({
                    'topic': data['topic'],
                    'description': issue_description,  # ì‹¤ì œ ì´ìŠˆ ë‚´ìš©
                    'what': data['what'],
                    'why_now': data['why_now'],
                    'context': data['context'],
                    'score': final_score,
                    'mention_count': mention_count,
                    'interest_score': interest_score,  # ê´€ì‹¬ë„ ì ìˆ˜ ì¶”ê°€
                    'source_diversity': source_diversity,
                    'max_sources': max_sources,
                    'sentiment': dominant_sentiment,
                    'content_quality': content_quality,
                    'temporal_relevance': temporal_relevance,
                    'collected_item_ids': list(data['collected_item_ids'])[:50],  # ìµœëŒ€ 50ê°œ
                    'analysis_ids': data['analysis_ids'],
                    # ì¶œì²˜ ì •ë³´ ì¶”ê°€
                    'top_source_types': [{'type': st, 'count': cnt} for st, cnt in top_source_types],
                    'top_source_names': [{'name': sn, 'count': cnt} for sn, cnt in top_source_names]
                })
            
            # ê´€ì‹¬ë„ ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬ (ê´€ì‹¬ë„ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ)
            ranked_issues.sort(key=lambda x: x['interest_score'], reverse=True)
            
            # ìƒìœ„ 10ìœ„ê¹Œì§€ë§Œ ë°˜í™˜ (5ë¶„ë§ˆë‹¤ ìˆ˜ì§‘ëœ ë°ì´í„°ë¡œ ê³„ì‚°ëœ ê´€ì‹¬ë„ ê¸°ì¤€)
            top_10_rankings = ranked_issues[:10]
            
            logger.info(f"âœ… ì´ìŠˆ ë­í‚¹ ê³„ì‚° ì™„ë£Œ: {len(ranked_issues)}ê°œ ì´ìŠˆ ì¤‘ ìƒìœ„ 10ìœ„ ì„ ì •")
            if top_10_rankings:
                logger.info("ğŸ† ìƒìœ„ 10ìœ„ ë­í‚¹:")
                for i, ranking in enumerate(top_10_rankings, 1):
                    logger.info(f"  {i}. {ranking['topic']} (ê´€ì‹¬ë„: {ranking['interest_score']:,}, ì ìˆ˜: {ranking['score']:.3f})")
            
            return top_10_rankings
            
        except Exception as e:
            logger.error(f"âŒ ì´ìŠˆ ë­í‚¹ ê³„ì‚° ì‹¤íŒ¨: {type(e).__name__} - {e}")
            import traceback
            traceback.print_exc()
            return []


async def save_issue_rankings(rankings: List[Dict[str, Any]], period_hours: int = 1) -> int:
    """
    ì´ìŠˆ ë­í‚¹ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        rankings: ë­í‚¹ëœ ì´ìŠˆ ë¦¬ìŠ¤íŠ¸
        period_hours: ë­í‚¹ ê¸°ê°„ (ì‹œê°„)
    
    Returns:
        ì €ì¥ëœ ë­í‚¹ ìˆ˜
    """
    if not rankings:
        return 0
    
    saved_count = 0
    async with AsyncSessionLocal() as session:
        try:
            from datetime import timezone
            period_start = datetime.now(timezone.utc) - timedelta(hours=period_hours)
            period_end = datetime.now(timezone.utc)
            
            # ê¸°ì¡´ ë­í‚¹ ì‚­ì œ (ê°™ì€ ê¸°ê°„ì˜ ë­í‚¹ì´ ìˆìœ¼ë©´)
            delete_query = select(IssueRanking).where(
                IssueRanking.period_start >= period_start - timedelta(hours=1)
            )
            existing = await session.execute(delete_query)
            for old_ranking in existing.scalars().all():
                await session.delete(old_ranking)
            
            # ìƒˆë¡œìš´ ë­í‚¹ ì €ì¥
            for rank, issue in enumerate(rankings, 1):
                # íŠ¸ë Œë“œ ë°©í–¥ ê³„ì‚°
                trend_direction = await calculate_trend_direction(
                    topic=issue['topic'],
                    current_interest=issue.get('interest_score', issue['mention_count']),
                    current_rank=rank,
                    session=session
                )
                
                ranking = IssueRanking(
                    topic=issue['topic'],
                    rank=rank,
                    score=issue['score'],
                    mention_count=issue.get('interest_score', issue['mention_count']),  # ê´€ì‹¬ë„ë¥¼ mention_countì— ì €ì¥
                    source_diversity=issue['source_diversity'],
                    trend_direction=trend_direction,  # ì´ì „ ë­í‚¹ê³¼ ë¹„êµí•˜ì—¬ ê³„ì‚°
                    period_start=period_start,
                    period_end=period_end,
                    description=issue.get('description', ''),
                    what=issue.get('what', ''),
                    why_now=issue.get('why_now', ''),
                    context=issue.get('context', '')
                )
                
                session.add(ranking)
                saved_count += 1
            
            await session.commit()
            logger.info(f"ğŸ’¾ ì´ìŠˆ ë­í‚¹ ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ")
            
        except Exception as e:
            await session.rollback()
            logger.error(f"âŒ ì´ìŠˆ ë­í‚¹ ì €ì¥ ì‹¤íŒ¨: {type(e).__name__} - {e}")
            import traceback
            traceback.print_exc()
            raise
    
    return saved_count


async def calculate_trend_direction(topic: str, current_interest: int, current_rank: int, session: AsyncSession) -> str:
    """
    ì´ì „ ë­í‚¹ê³¼ ë¹„êµí•˜ì—¬ íŠ¸ë Œë“œ ë°©í–¥ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        topic: íŠ¸ë Œë“œ í† í”½
        current_interest: í˜„ì¬ ê´€ì‹¬ë„ ì ìˆ˜
        current_rank: í˜„ì¬ ìˆœìœ„
        session: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
    
    Returns:
        íŠ¸ë Œë“œ ë°©í–¥ ('up', 'down', 'stable')
    """
    try:
        # ì´ì „ ë­í‚¹ ì¡°íšŒ (ìµœê·¼ 3ê°œ ì£¼ê¸°, ì•½ 15ë¶„)
        from datetime import timezone
        cutoff_time = datetime.now(timezone.utc) - timedelta(minutes=20)
        
        previous_rankings_query = select(IssueRanking).where(
            and_(
                IssueRanking.topic == topic,
                IssueRanking.period_end < cutoff_time
            )
        ).order_by(desc(IssueRanking.period_end)).limit(3)
        
        previous_result = await session.execute(previous_rankings_query)
        previous_rankings = list(previous_result.scalars().all())
        
        if not previous_rankings:
            return 'stable'  # ì´ì „ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ stable
        
        # ê°€ì¥ ìµœê·¼ ì´ì „ ë­í‚¹ ì‚¬ìš©
        previous_ranking = previous_rankings[0]
        previous_interest = previous_ranking.mention_count or 0
        previous_rank = previous_ranking.rank or 999
        
        # ê´€ì‹¬ë„ ë³€í™”ìœ¨ ê³„ì‚°
        if previous_interest > 0:
            interest_change_rate = ((current_interest - previous_interest) / previous_interest) * 100
        else:
            interest_change_rate = 100 if current_interest > 0 else 0
        
        # ìˆœìœ„ ë³€í™” ê³„ì‚° (ì–‘ìˆ˜ë©´ ìƒìŠ¹, ìŒìˆ˜ë©´ í•˜ë½)
        rank_change = previous_rank - current_rank
        
        # íŠ¸ë Œë“œ ë°©í–¥ ê²°ì •
        # ê´€ì‹¬ë„ê°€ 50% ì´ìƒ ì¦ê°€í•˜ê±°ë‚˜ ìˆœìœ„ê°€ 3ê³„ë‹¨ ì´ìƒ ìƒìŠ¹í•˜ë©´ 'up'
        if interest_change_rate >= 50 or rank_change >= 3:
            return 'up'
        # ê´€ì‹¬ë„ê°€ 30% ì´ìƒ ê°ì†Œí•˜ê±°ë‚˜ ìˆœìœ„ê°€ 3ê³„ë‹¨ ì´ìƒ í•˜ë½í•˜ë©´ 'down'
        elif interest_change_rate <= -30 or rank_change <= -3:
            return 'down'
        else:
            return 'stable'
            
    except Exception as e:
        logger.warning(f"âš ï¸ íŠ¸ë Œë“œ ë°©í–¥ ê³„ì‚° ì‹¤íŒ¨ ({topic}): {e}")
        return 'stable'


async def detect_surge_trends(limit: int = 5) -> List[Dict[str, Any]]:
    """
    ê¸‰ìƒìŠ¹ íŠ¸ë Œë“œë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
    
    ì¡°ê±´:
    - ê´€ì‹¬ë„ê°€ 2ë°° ì´ìƒ ì¦ê°€
    - ìˆœìœ„ê°€ 5ê³„ë‹¨ ì´ìƒ ìƒìŠ¹
    - ìµœê·¼ 3ê°œ ë­í‚¹ ì£¼ê¸°(15ë¶„) ë‚´ ê¸‰ìƒìŠ¹
    
    Args:
        limit: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜
    
    Returns:
        ê¸‰ìƒìŠ¹ íŠ¸ë Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    async with AsyncSessionLocal() as session:
        try:
            from datetime import timezone
            from sqlalchemy import or_
            
            # ìµœê·¼ ë­í‚¹ ê°€ì ¸ì˜¤ê¸°
            latest_period = await session.execute(
                select(func.max(IssueRanking.period_start))
            )
            latest_start = latest_period.scalar()
            
            if not latest_start:
                return []
            
            # í˜„ì¬ ë­í‚¹ ê°€ì ¸ì˜¤ê¸°
            current_rankings_query = select(IssueRanking).where(
                IssueRanking.period_start == latest_start
            ).order_by(IssueRanking.rank)
            
            current_result = await session.execute(current_rankings_query)
            current_rankings = list(current_result.scalars().all())
            
            if not current_rankings:
                return []
            
            surge_trends = []
            
            # ê° í˜„ì¬ ë­í‚¹ì— ëŒ€í•´ ì´ì „ ë­í‚¹ê³¼ ë¹„êµ
            for current_ranking in current_rankings:
                topic = current_ranking.topic
                current_interest = current_ranking.mention_count or 0
                current_rank = current_ranking.rank
                
                # ìµœê·¼ 3ê°œ ë­í‚¹ ì£¼ê¸° ì¡°íšŒ (ì•½ 15ë¶„)
                cutoff_time = datetime.now(timezone.utc) - timedelta(minutes=20)
                
                previous_rankings_query = select(IssueRanking).where(
                    and_(
                        IssueRanking.topic == topic,
                        IssueRanking.period_end < cutoff_time
                    )
                ).order_by(desc(IssueRanking.period_end)).limit(3)
                
                previous_result = await session.execute(previous_rankings_query)
                previous_rankings = list(previous_result.scalars().all())
                
                if not previous_rankings:
                    continue
                
                # ê°€ì¥ ì˜¤ë˜ëœ ì´ì „ ë­í‚¹ê³¼ ë¹„êµ (15ë¶„ ì „)
                oldest_previous = previous_rankings[-1]
                previous_interest = oldest_previous.mention_count or 0
                previous_rank = oldest_previous.rank or 999
                
                # ê´€ì‹¬ë„ ë³€í™”ìœ¨ ê³„ì‚°
                if previous_interest > 0:
                    interest_change_rate = ((current_interest - previous_interest) / previous_interest) * 100
                    interest_multiplier = current_interest / previous_interest
                else:
                    interest_change_rate = 100 if current_interest > 0 else 0
                    interest_multiplier = 2.0 if current_interest > 0 else 1.0
                
                # ìˆœìœ„ ë³€í™” ê³„ì‚°
                rank_change = previous_rank - current_rank  # ì–‘ìˆ˜ë©´ ìƒìŠ¹
                
                # ê¸‰ìƒìŠ¹ ì¡°ê±´ í™•ì¸
                is_surge = False
                surge_reason = []
                
                if interest_multiplier >= 2.0:  # ê´€ì‹¬ë„ 2ë°° ì´ìƒ ì¦ê°€
                    is_surge = True
                    surge_reason.append(f"ê´€ì‹¬ë„ {interest_multiplier:.1f}ë°° ì¦ê°€")
                
                if rank_change >= 5:  # ìˆœìœ„ 5ê³„ë‹¨ ì´ìƒ ìƒìŠ¹
                    is_surge = True
                    surge_reason.append(f"ìˆœìœ„ {rank_change}ê³„ë‹¨ ìƒìŠ¹")
                
                if interest_change_rate >= 100:  # ê´€ì‹¬ë„ 100% ì´ìƒ ì¦ê°€
                    is_surge = True
                    surge_reason.append(f"ê´€ì‹¬ë„ {interest_change_rate:.0f}% ì¦ê°€")
                
                if is_surge:
                    surge_trends.append({
                        'topic': topic,
                        'current_rank': current_rank,
                        'previous_rank': previous_rank,
                        'rank_change': rank_change,
                        'current_interest': current_interest,
                        'previous_interest': previous_interest,
                        'interest_change_rate': interest_change_rate,
                        'interest_multiplier': interest_multiplier,
                        'surge_reason': ', '.join(surge_reason),
                        'description': current_ranking.description or '',
                        'what': current_ranking.what or '',
                        'why_now': current_ranking.why_now or '',
                        'context': current_ranking.context or '',
                    })
            
            # ê´€ì‹¬ë„ ë³€í™”ìœ¨ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ê°€ì¥ ê¸‰ìƒìŠ¹í•œ ìˆœì„œ)
            surge_trends.sort(key=lambda x: x['interest_change_rate'], reverse=True)
            
            logger.info(f"ğŸ”¥ ê¸‰ìƒìŠ¹ íŠ¸ë Œë“œ {len(surge_trends)}ê°œ ê°ì§€")
            
            return surge_trends[:limit]
            
        except Exception as e:
            logger.error(f"âŒ ê¸‰ìƒìŠ¹ íŠ¸ë Œë“œ ê°ì§€ ì‹¤íŒ¨: {type(e).__name__} - {e}")
            import traceback
            traceback.print_exc()
            return []


async def get_top_rankings(limit: int = 10) -> List[IssueRanking]:
    """
    ìµœì‹  ì´ìŠˆ ë­í‚¹ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        limit: ê°€ì ¸ì˜¬ ìµœëŒ€ ê°œìˆ˜
    
    Returns:
        IssueRanking ë¦¬ìŠ¤íŠ¸
    """
    async with AsyncSessionLocal() as session:
        try:
            # ê°€ì¥ ìµœê·¼ ê¸°ê°„ì˜ ë­í‚¹ ê°€ì ¸ì˜¤ê¸°
            latest_period = await session.execute(
                select(func.max(IssueRanking.period_start))
            )
            latest_start = latest_period.scalar()
            
            if not latest_start:
                return []
            
            # í•´ë‹¹ ê¸°ê°„ì˜ ë­í‚¹ ê°€ì ¸ì˜¤ê¸°
            query = select(IssueRanking).where(
                IssueRanking.period_start == latest_start
            ).order_by(IssueRanking.rank).limit(limit)
            
            result = await session.execute(query)
            return list(result.scalars().all())
            
        except Exception as e:
            logger.error(f"âŒ ë­í‚¹ ì¡°íšŒ ì‹¤íŒ¨: {type(e).__name__} - {e}")
            return []


