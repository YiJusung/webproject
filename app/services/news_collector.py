"""
ë‰´ìŠ¤ ì‚¬ì´íŠ¸ RSS í”¼ë“œë¥¼ ìˆ˜ì§‘í•˜ëŠ” ëª¨ë“ˆ
"""
import httpx
import xml.etree.ElementTree as ET
import logging
from datetime import datetime
from typing import List, Dict

logger = logging.getLogger("hourly_pulse")

async def fetch_news_rss(url: str, source_name: str) -> List[Dict[str, str]]:
    """
    RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Args:
        url: RSS í”¼ë“œ URL
        source_name: ë‰´ìŠ¤ ì†ŒìŠ¤ ì´ë¦„ (ì˜ˆ: "BBC", "CNN")
    
    Returns:
        ë‰´ìŠ¤ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ (title, link, pubDate í¬í•¨)
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    logger.info(f"ğŸ“° {source_name} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    
    async with httpx.AsyncClient(timeout=10.0, headers=headers) as client:
        try:
            response = await client.get(url)
            response.raise_for_status()
            
            root = ET.fromstring(response.text)
            news_items = []
            
            # RSS êµ¬ì¡°: channel -> item
            # RSS ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì²˜ë¦¬ (ì¼ë¶€ ì‚¬ì´íŠ¸ëŠ” í™•ì¥ í•„ë“œ ì‚¬ìš©)
            namespaces = {
                'slash': 'http://purl.org/rss/1.0/modules/slash/',
                'wfw': 'http://wellformedweb.org/CommentAPI/',
                'content': 'http://purl.org/rss/1.0/modules/content/',
                'dc': 'http://purl.org/dc/elements/1.1/'
            }
            
            for item in root.findall(".//item"):
                title_elem = item.find("title")
                link_elem = item.find("link")
                pub_date_elem = item.find("pubDate")
                description_elem = item.find("description")
                
                # ëŒ“ê¸€ ìˆ˜ í•„ë“œ ì°¾ê¸° (ë‹¤ì–‘í•œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì‹œë„)
                comments_count = 0
                # í‘œì¤€ í•„ë“œ
                comments_elem = item.find("comments")
                if comments_elem is not None and comments_elem.text:
                    try:
                        comments_count = int(comments_elem.text)
                    except (ValueError, TypeError):
                        pass
                
                # Slash ë„¤ì„ìŠ¤í˜ì´ìŠ¤ (ì¼ë¶€ ì‚¬ì´íŠ¸ ì‚¬ìš©)
                if comments_count == 0:
                    slash_comments = item.find("slash:comments", namespaces)
                    if slash_comments is not None and slash_comments.text:
                        try:
                            comments_count = int(slash_comments.text)
                        except (ValueError, TypeError):
                            pass
                
                # ê¸°íƒ€ í™•ì¥ í•„ë“œ ì‹œë„
                if comments_count == 0:
                    # ëª¨ë“  ìì‹ ìš”ì†Œì—ì„œ "comment"ê°€ í¬í•¨ëœ í•„ë“œ ì°¾ê¸°
                    for child in item:
                        tag = child.tag.lower() if hasattr(child, 'tag') else ''
                        text = child.text if hasattr(child, 'text') and child.text else ''
                        if 'comment' in tag and text:
                            try:
                                comments_count = int(text)
                                break
                            except (ValueError, TypeError):
                                pass
                
                if title_elem is not None and title_elem.text:
                    news_item = {
                        "source": source_name,
                        "title": title_elem.text.strip(),
                        "url": link_elem.text if link_elem is not None else "",
                        "published": pub_date_elem.text if pub_date_elem is not None else "",
                        "description": description_elem.text if description_elem is not None else "",
                        "comments": comments_count,  # ëŒ“ê¸€ ìˆ˜ ì¶”ê°€ (ìˆìœ¼ë©´)
                        "collected_at": datetime.now().isoformat()
                    }
                    news_items.append(news_item)
            
            logger.info(f"âœ… {source_name} ìˆ˜ì§‘ ì„±ê³µ! {len(news_items)}ê°œ ê¸°ì‚¬ ë°œê²¬")
            return news_items[:50]  # ìƒìœ„ 50ê°œ ë°˜í™˜ (10 -> 50ìœ¼ë¡œ ì¦ê°€, 4K RPM í™œìš©)
            
        except Exception as e:
            logger.error(f"âŒ {source_name} ìˆ˜ì§‘ ì‹¤íŒ¨: {type(e).__name__} - {e}")
            return []


async def fetch_multiple_news_sources() -> List[Dict[str, str]]:
    """
    ì—¬ëŸ¬ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    news_sources = [
        # êµ­ì œ ë‰´ìŠ¤ (ì‘ë™ í™•ì¸ë¨)
        ("https://feeds.bbci.co.uk/news/rss.xml", "BBC"),
        ("https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml", "NYTimes"),
        ("https://rss.cbc.ca/lineup/topstories.xml", "CBC"),
        ("https://feeds.washingtonpost.com/rss/world", "WashingtonPost"),
        ("https://www.theguardian.com/world/rss", "TheGuardian"),
        
        # ê¸°ìˆ  ë‰´ìŠ¤ (ì‘ë™ í™•ì¸ë¨)
        ("https://hnrss.org/frontpage", "HackerNews"),
        ("https://techcrunch.com/feed/", "TechCrunch"),
        ("https://www.theverge.com/rss/index.xml", "TheVerge"),
        ("https://www.wired.com/feed/rss", "Wired"),
        ("https://feeds.arstechnica.com/arstechnica/index", "ArsTechnica"),
        ("https://www.engadget.com/rss.xml", "Engadget"),
        
        # ê²½ì œ ë‰´ìŠ¤
        ("https://feeds.bloomberg.com/markets/news.rss", "Bloomberg"),
    ]
    
    all_news = []
    for url, source in news_sources:
        news = await fetch_news_rss(url, source)
        all_news.extend(news)
    
    return all_news

